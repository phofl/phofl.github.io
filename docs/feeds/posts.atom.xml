<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Hoefler - posts</title><link href="https://phofl.github.io/" rel="alternate"></link><link href="https://phofl.github.io/feeds/posts.atom.xml" rel="self"></link><id>https://phofl.github.io/</id><updated>2023-08-04T00:00:00+02:00</updated><entry><title>High Level Query Optimization in Dask</title><link href="https://phofl.github.io/high-level-query-optimization-in-dask.html" rel="alternate"></link><published>2023-08-04T00:00:00+02:00</published><updated>2023-08-04T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-08-04:/high-level-query-optimization-in-dask.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/dask-expr/dask-expr-introduction-title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dask DataFrame doesn't currently optimize your code for you (like Spark or a SQL database would). 
This means that users waste a lot of computation. Let's look at a common example
which looks ok at first glance, but is actually pretty inefficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd

df = dd …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/dask-expr/dask-expr-introduction-title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dask DataFrame doesn't currently optimize your code for you (like Spark or a SQL database would). 
This means that users waste a lot of computation. Let's look at a common example
which looks ok at first glance, but is actually pretty inefficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,  # unnecessarily reads all rows and columns
)
result = (
    df[df.hvfhs_license_num == &amp;quot;HV0003&amp;quot;]    # could push the filter into the read parquet call
    .sum(numeric_only=True)
    [&amp;quot;tips&amp;quot;]                                # should read only necessary columns
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make this run much faster with a few simple steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
    columns=[&amp;quot;tips&amp;quot;],
)
result = df.tips.sum()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently, Dask DataFrame wouldn't optimize this for you, but a new effort that is built around
logical query planning in Dask DataFrame will do this for you. This article introduces some of
those changes that are developed in &lt;a href="https://github.com/dask-contrib/dask-expr"&gt;dask-expr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can install and try &lt;code&gt;dask-expr&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pip install dask-expr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are using the &lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;NYC Taxi&lt;/a&gt; 
dataset in this post.&lt;/p&gt;
&lt;h2 id="dask-expressions"&gt;Dask Expressions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/dask-contrib/dask-expr"&gt;Dask expressions&lt;/a&gt; provides a logical query planning layer on 
top of Dask DataFrames. Let's look at our initial example and investigate how we can improve the efficiency
through a query optimization layer. As noted initially, there are a couple of things that aren't ideal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are reading all rows into memory instead of filtering while reading the parquet files.&lt;/li&gt;
&lt;li&gt;We are reading all columns into memory instead of only the columns that are necessary.&lt;/li&gt;
&lt;li&gt;We are applying the filter and the aggregation onto all columns instead of only &lt;code&gt;"tips"&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The query optimization layer from &lt;code&gt;dask-expr&lt;/code&gt; can help us with that. It will look at this expression
and determine that not all rows are needed. An intermediate layer will transpile the filter into
a valid filter-expression for &lt;code&gt;read_parquet&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
)
result = df.sum(numeric_only=True)[&amp;quot;tips&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This still reads every column into memory and will compute the sum of every numeric column. The 
next optimization step is to push the column selection into the &lt;code&gt;read_parquet&lt;/code&gt; call as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    columns=[&amp;quot;tips&amp;quot;],
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
)
result = df.sum(numeric_only=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a basic example that you could rewrite by hand. Use cases that are closer to real
workflows might potentially have hundreds of columns, which makes rewriting them very strenuous
if you need a non-trivial subset of them.&lt;/p&gt;
&lt;p&gt;Let's take a look at how we can achieve this. &lt;code&gt;dask-expr&lt;/code&gt; records the expression as given by the
user in an expression tree:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result.pprint()

Projection: columns='tips'
  Sum: numeric_only=True
    Filter:
      ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/'
      EQ: right='HV0003'
        Projection: columns='hvfhs_license_num'
          ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tree represents the expression as is. We can observe that we would read the whole dataset into
memory before we apply the projections and filters. One observation of note: It seems like we
are reading the dataset twice, but Dask is able to fuse tasks that are doing the same to avoid
computing these things twice. Let's reorder the expression to make it more efficient:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result.simplify().pprint()

Sum: numeric_only=True
  ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/' 
               columns=['tips'] 
               filters=[('hvfhs_license_num', '==', 'HV0003')] 

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks quite a bit simpler. &lt;code&gt;dask-expr&lt;/code&gt; reordered the query and pushed the filter and the column
projection into the &lt;code&gt;read_parquet&lt;/code&gt; call. We were able to remove quite a few steps from our expression
tree and make the remaining expressions more efficient as well. This represents the steps that
we did manually in the beginning. &lt;code&gt;dask-expr&lt;/code&gt; performs these steps for arbitrary many columns without
increasing the burden on the developers.&lt;/p&gt;
&lt;p&gt;These are only the two most common and easy to illustrate optimization techniques from &lt;code&gt;dask-expr&lt;/code&gt;. 
Some other useful optimizations are already available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;len(...)&lt;/code&gt; will only use the Index to compute the length; additionally we can ignore many operations
  that won't change the shape of a DataFrame, like a &lt;code&gt;replace&lt;/code&gt; call.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;set_index&lt;/code&gt; and &lt;code&gt;sort_values&lt;/code&gt; won't eagerly trigger computations.&lt;/li&gt;
&lt;li&gt;Better informed selection of &lt;code&gt;merge&lt;/code&gt; algorithms.&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are still adding more optimization techniques to make Dask DataFrame queries more efficient.&lt;/p&gt;
&lt;h2 id="try-it-out"&gt;Try it out&lt;/h2&gt;
&lt;p&gt;The project is in a state where interested users should try it out. We published a couple of 
releases. The API covers a big chunk of the Dask DataFrame API, and we keep adding more. 
We have already observed very impressive performance improvements for workflows that would benefit
from query optimization. Memory usage is down for these workflows as well.&lt;/p&gt;
&lt;p&gt;We are very much looking for feedback and potential avenues to improve the library. Please give it
a shot and share your experience with us.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dask-expr&lt;/code&gt; is not integrated into the main Dask DataFrame implementation yet. You can install it
with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pip install dask-expr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The API is very similar to what Dask DataFrame provides. It exposes mostly the same methods as
Dask DataFrame does. You can use the same methods in most cases.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import dask_expr as dd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find a list of supported operations in the 
&lt;a href="https://github.com/dask-contrib/dask-expr#api-coverage"&gt;Readme&lt;/a&gt;. This project is still very much
in progress. The API might change without warning. We are aiming for weekly releases to push new
features out as fast as possible.&lt;/p&gt;
&lt;h2 id="why-are-we-adding-this-now"&gt;Why are we adding this now?&lt;/h2&gt;
&lt;p&gt;Historically, Dask focused on flexibility and smart scheduling instead of query optimization. 
The distributed scheduler built into Dask uses sophisticated algorithms to ensure ideal scheduling
of individual tasks. It tries to ensure that your resources are utilized as efficient as possible.
The graph construction process enables Dask users to build very
flexible and complicated graphs that reach beyond SQL operations. The flexibility that is provided
by the &lt;a href="https://docs.dask.org/en/latest/futures.html"&gt;Dask futures API&lt;/a&gt; requires very intelligent
algorithms, but it enables users to build highly sophisticated graphs. The following picture shows
the graph for a credit risk model:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask-expr/graph_credit_risk_model.png"&gt;&lt;/p&gt;
&lt;p&gt;The nature of the powerful scheduler and the physical optimizations enables us to build very
complicated programs that will then run efficiently. Unfortunately, the nature of these optimizations 
does not enable us to avoid scheduling work that is not necessary. This is where the current effort
to build high level query optimization into Dask comes in.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Dask comes with a very smart distributed scheduler but without much logical query planning. This
is something we are rectifying now through building a high level query optimizer into Dask 
DataFrame. We expect to improve performance and reduce memory usage for an average Dask workflow.&lt;/p&gt;
&lt;p&gt;This API is read for interested users to play around with. It covers a good chunk of the DataFrame
API. The library is under active development, we expect to add many more interesting things over
the coming weeks and months. &lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="query optimizer"></category><category term="performance"></category></entry><entry><title>How to Train a Neural Network on a GPU in the Cloud with coiled functions</title><link href="https://phofl.github.io/coiled-functions-pytorch.html" rel="alternate"></link><published>2023-07-24T00:00:00+02:00</published><updated>2023-07-24T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-07-24:/coiled-functions-pytorch.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going through the pains of setting up environments in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post will provide an example how to utilize &lt;code&gt;coiled functions&lt;/code&gt; to seamlessly train a 
&lt;strong&gt;neural network&lt;/strong&gt; on a GPU that is hosted in the cloud.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We have to start with creating a model on our local machine before we can start worrying about
training it. This blog post is not dedicated to figuring out a fancy mode, we will utilize the
&lt;code&gt;Net&lt;/code&gt; model that is given in the &lt;a href="https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html#training-your-pytorch-model"&gt;PyTorch tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can simply add the model definition to our python script. There is no need to do anything
different. Similarly, we will use the transformer that is given there as well.&lt;/p&gt;
&lt;p&gt;The next step is creating a function that we can use to train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train(transform):
    device = torch.device(&amp;quot;cpu&amp;quot;)
    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now train our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will train our model on the CPU of our local machine. The training is reasonably quick for such
a small model, but training time will grow exponentially as our model gets larger or if we are
using a significantly bigger dataset. Training the model on the CPU won't be sufficient anymore.
Additionally, there are a lot of machines out there that don't have GPUs built into them. For example, I'm using a MacBook Pro with an M2 CPU, which means my machine doesn't support &lt;code&gt;cuda&lt;/code&gt;.
Consequently, we need a different solution to make these steps accessible for folks who don't have
access to a local GPU.&lt;/p&gt;
&lt;h2 id="using-coiled-functions-to-train-the-model-on-a-cloud-hosted-gpu"&gt;Using &lt;code&gt;coiled functions&lt;/code&gt; to train the model on a cloud-hosted GPU&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;Coiled functions&lt;/a&gt; come into the equation if you 
need access to resources that aren't available
locally. Coiled can connect to AWS or GCP and thus, use all resources that are available there.
We will go through the necessary steps to train our model on a GPU that is hosted on AWS instead
of our local CPU. &lt;/p&gt;
&lt;p&gt;The first step includes defining a Python environment to run our computations. We simply include
PyTorch, CUDA, and Coiled, that's it. Generally, you should use the same Python version that is
installed locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

coiled.create_software_environment(
    name=&amp;quot;pytorch&amp;quot;,
    conda={
        &amp;quot;channels&amp;quot;: [&amp;quot;pytorch&amp;quot;, &amp;quot;nvidia&amp;quot;, &amp;quot;conda-forge&amp;quot;, &amp;quot;defaults&amp;quot;],
        &amp;quot;dependencies&amp;quot;: [
            &amp;quot;python=3.11&amp;quot;,
            &amp;quot;coiled&amp;quot;,
            &amp;quot;pytorch&amp;quot;,
            &amp;quot;torchvision&amp;quot;,
            &amp;quot;torchaudio&amp;quot;,
            &amp;quot;cudatoolkit&amp;quot;,
            &amp;quot;pynvml&amp;quot;,
        ],
    },
    gpu_enabled=True,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled will create a Python environment for you. This step is only necessary when running your 
script for the first time. The resulting environment is cached which makes further runs more
efficient.&lt;/p&gt;
&lt;p&gt;The next step is adding the &lt;code&gt;@coiled.run&lt;/code&gt; decorator to our training
function that tells our program we want to execute said function on a machine in the cloud.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;, # GPU instance type
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, we have to tell PyTorch that we want to train the model on the GPU.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train():
    import torch
    # tell PyTorch to use the GPU
    device = torch.device(&amp;quot;cuda:0&amp;quot;)
    ...
    return net.to(torch.device(&amp;quot;cpu&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting this all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;,
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
def train(transform):
    import torch
    device = torch.device(&amp;quot;cuda:0&amp;quot;)

    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net.to(torch.device(&amp;quot;cpu&amp;quot;))


if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's take a brief look at the arguments to &lt;code&gt;coiled.run()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;worker_vm_type&lt;/code&gt;: This specifies the type of &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;EC2 instance&lt;/a&gt;.
  We are looking for an instance that has a GPU attached to it. The G5 family has Nvidia GPUs
  attached to it. The smallest version is sufficient for our example, but you can choose instances
  with up to 8 GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;region&lt;/code&gt;: The region specifies the AWS region that our VM is started in. We observed that &lt;code&gt;"us-west-2"&lt;/code&gt;
  is a region where GPUs are easier to get. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;software&lt;/code&gt;: This specifies the coiled software environment that is installed. This corresponds
  to the environment that we previously created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;coiled.run()&lt;/code&gt; will now start a VM in AWS with the specified EC2 instance. The VM is normally up
and running in 1-2 minutes. The previously specified Python environment is installed automatically.
Coiled executes the function on said VM. Inputs of your function are serialized and sent to the VM
as well. It makes sense to download the training data on the VM to reduce time that is spent sending
data to AWS. The function returns our model back to our local machine so that we can use it locally
without depending on AWS.&lt;/p&gt;
&lt;p&gt;Coiled will shut down the VM immediately after the Python interpreter finishes. This is mostly to
reduce costs. You can specify a certain amount of time that the VM is kept alive through 
&lt;code&gt;keepalive="5 minutes"&lt;/code&gt;. This ensures that new local runs can connect to the same VM avoiding
the boot time of up to 2 minutes; we call this a warm start.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;coiled functions&lt;/code&gt; enables you to seamlessly port the training process for a neural network
from your local machine to AWS or GCP. This grants everyone access to multiple GPUs or huge
machines independent of the local machine that is actually used. Training a neural network on a
GPU becomes as easy as adding a decorator to the training function.&lt;/p&gt;</content><category term="posts"></category><category term="coiled"></category><category term="jobs"></category><category term="gpu"></category><category term="machine learning"></category></entry><entry><title>pandas Internals Explained</title><link href="https://phofl.github.io/pandas-internals.html" rel="alternate"></link><published>2023-07-21T00:00:00+02:00</published><updated>2023-07-21T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-07-21:/pandas-internals.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Explaining the pandas data model and its advantages&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;pandas enables you to choose between different types of arrays to represent the data of your
DataFrame. Historically, most DataFrames are backed by NumPy arrays. &lt;a href="https://medium.com/gitconnected/welcoming-pandas-2-0-194094e4275b"&gt;pandas 2.0 introduced the 
option to use PyArrow arrays&lt;/a&gt; as a storage format. 
There exists …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Explaining the pandas data model and its advantages&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;pandas enables you to choose between different types of arrays to represent the data of your
DataFrame. Historically, most DataFrames are backed by NumPy arrays. &lt;a href="https://medium.com/gitconnected/welcoming-pandas-2-0-194094e4275b"&gt;pandas 2.0 introduced the 
option to use PyArrow arrays&lt;/a&gt; as a storage format. 
There exists an intermediate layer between these arrays and your DataFrame, &lt;code&gt;Blocks&lt;/code&gt; and the
&lt;code&gt;BlockManager&lt;/code&gt;. We will take a look at how this layer orchestrates the different arrays, basically
what's behind &lt;code&gt;pd.DataFrame()&lt;/code&gt;. We will try to answer all questions you might have about pandas 
internals.&lt;/p&gt;
&lt;p&gt;The post will introduce some terminology that is necessary to understand how Copy-on-Write works,
which is something I'll write about next.&lt;/p&gt;
&lt;h2 id="pandas-data-structure"&gt;pandas data structure&lt;/h2&gt;
&lt;p&gt;A &lt;code&gt;DataFrame&lt;/code&gt; is usually backed by some sort of array, e.g. a NumPy array or 
pandas ExtensionArray. These arrays store the data of the DataFrame. pandas adds an intermediate 
layer, called &lt;code&gt;Block&lt;/code&gt; and &lt;code&gt;BlockManager&lt;/code&gt; that orchestrate these arrays to make operations as 
efficient as possible. This is one of the reasons why methods that operate on multiple columns can 
be very fast in pandas. Let's look a bit more into the details of these layers.&lt;/p&gt;
&lt;h3 id="arrays"&gt;Arrays&lt;/h3&gt;
&lt;p&gt;The actual data of a DataFrame can be stored in a set of NumPy arrays or pandas ExtensionArrays. 
This layer generally dispatches to the underlying implementation, e.g. it will utilize the NumPy 
API if the data is stored in NumPy arrays. pandas simply stores the data in them and calls its 
methods without enriching the interace. You can read up on pandas 
ExtensionArrays &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.extensions.ExtensionArray.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;NumPy arrays are normally 2-dimensional, which offers a bunch of performance advantages that we
will take a look at later. pandas ExtensionArrays are mostly one-dimensional data structures as
of right now. This makes things a bit more predictable but has some drawbacks when looking at
performance in a specific set of operations. ExtensionArrays enable DataFrames that are backed by
PyArrow arrays among other dtypes.&lt;/p&gt;
&lt;h3 id="blocks"&gt;Blocks&lt;/h3&gt;
&lt;p&gt;A DataFrame normally consists of a set of columns that are represented by at least one array, 
normally you'll have a collection of arrays since one array can only store one specific dtype.
These arrays store your data but don't have any information about which columns they are
representing. Every array from your DataFrame is wrapped by one corresponding &lt;code&gt;Block&lt;/code&gt;. Blocks
add some additional information to these arrays like the column locations that are represented
by this Block. Blocks serve as a layer around the actual arrays that can be enriched with utility methods
that are necessary for pandas operations.&lt;/p&gt;
&lt;p&gt;When an actual operation is executed on a DataFrame, the Block ensures that the method dispatches
to the underlying array, e.g. if you call &lt;code&gt;astype&lt;/code&gt;, it will make sure that this operation is
called on the array.&lt;/p&gt;
&lt;p&gt;This layer does not have any information about the other columns in the DataFrame. It is a stand-alone
object.&lt;/p&gt;
&lt;h3 id="blockmanager"&gt;BlockManager&lt;/h3&gt;
&lt;p&gt;As the name suggests, the &lt;code&gt;BlockManager&lt;/code&gt; orchestrates all Blocks that are connected to one 
DataFrame. It holds the Blocks itself and information about your DataFrame's axes, e.g. column names
and Index labels. Most importantly, it dispatches most operations to the actual Blocks.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.replace(...)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The BlockManager ensures that &lt;code&gt;replace&lt;/code&gt; is executed on every Block.&lt;/p&gt;
&lt;h2 id="what-is-a-consolidated-dataframe"&gt;What is a consolidated DataFrame&lt;/h2&gt;
&lt;p&gt;We are assuming that the DataFrames is backed by NumPy dtypes, e.g. that it's data can be stored
as two-dimensional arrays.&lt;/p&gt;
&lt;p&gt;When a DataFrame is constructed, pandas mostly ensures that there is only one Block per dtype.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame(
    {
        &amp;quot;a&amp;quot;: [1, 2, 3],
        &amp;quot;b&amp;quot;: [1.5, 2.5, 3.5],
        &amp;quot;c&amp;quot;: [10, 11, 12],
        &amp;quot;d&amp;quot;: [10.5, 11.5, 12.5],
    }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This DataFrame has 4 columns which are represented by 2 arrays, one of the arrays stores the integer
dtypes while the other stores the float dtypes. This is a &lt;em&gt;consolidated&lt;/em&gt; DataFrame.&lt;/p&gt;
&lt;p&gt;Now let's add a new column to this DataFrame:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;new&amp;quot;] = 100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will have the same dtype as our existing column &lt;code&gt;"a"&lt;/code&gt; and &lt;code&gt;"c"&lt;/code&gt;. There are now two potential
ways of moving forward:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add the new column to the existing array that holds the integer columns&lt;/li&gt;
&lt;li&gt;Create a new array that only stores the new column.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first option would require us to add a new column to the existing array. This would require copying
the data since NumPy does not support this operation without a copy. This is obviously a pretty steep
cost for simply adding one column.&lt;/p&gt;
&lt;p&gt;The second option adds a third array to our collection of arrays. Apart from this, no additional 
operation is necessary. This is very cheap. We now have 2 Blocks that store integer data. This is
a DataFrame that is not consolidated.&lt;/p&gt;
&lt;p&gt;These differences don't matter much as long as you are only operating on a per-column basis. It
will impact the performance of your operations as soon as they operate on multiple columns.
For example, performing any &lt;code&gt;axis=1&lt;/code&gt; operation will transpose the data of your DataFrame. 
Transposing is generally zero-copy if performed on a DataFrame that is backed by a single NumPy
array. This is no longer true if every column is backed by a different array and hence, will incur
performance penalties.&lt;/p&gt;
&lt;p&gt;It will also require a copy when you want to get all integer columns from your DataFrame as a 
NumPy array.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[[&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;new&amp;quot;]].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a copy since the results have to be stored in a single NumPy array. It returns
a view on a consolidated DataFrame, which makes this very cheap.&lt;/p&gt;
&lt;p&gt;Previous versions often caused an internal consolidation for certain methods, which in turn caused 
unpredictable performance behavior. Methods like &lt;code&gt;reset_index&lt;/code&gt; were triggering a completely
unnecessary consolidation. These were mostly removed over the last couple of releases.&lt;/p&gt;
&lt;p&gt;Summarizing, a consolidated DataFrame is generally better than an unconsolidated one, but the 
difference depends heavily on the type of operation you want to execute.&lt;/p&gt;
&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We took a brief look behind the scenes of a pandas DataFrame. We learned what &lt;code&gt;Blocks&lt;/code&gt; and
&lt;code&gt;BlockManagers&lt;/code&gt; are and how they orchestrate your DataFrame. These terms will prove valuable
when we take a look behind the scenes of 
&lt;a href="https://medium.com/towards-data-science/a-solution-for-inconsistencies-in-indexing-operations-in-pandas-b76e10719744"&gt;Copy-on-Write&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out to share your thoughts and feedback.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category></entry><entry><title>Dask performance benchmarking put to the test: Fixing a pandas bottleneck</title><link href="https://phofl.github.io/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html" rel="alternate"></link><published>2023-06-28T00:00:00+02:00</published><updated>2023-06-28T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-28:/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With an impact of 40% increased runtime, it blocked the release planned for the next day!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/performance-regression.png"&gt;&lt;/p&gt;
&lt;p&gt;Luckily, the other attendees still focused on coffee and breakfast, so we commandeered an abandoned table next to our booth and got to work.&lt;/p&gt;
&lt;h2 id="performance-testing-at-coiled"&gt;Performance testing at Coiled&lt;/h2&gt;
&lt;p&gt;The performance problem &lt;a href="https://github.com/coiled/benchmarks/issues/840"&gt;had been flagged&lt;/a&gt; by the automated performance testing for Dask that we developed at &lt;a href="https://www.coiled.io/?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Coiled&lt;/a&gt;.
If you have not read Guido Imperiale's &lt;a href="https://blog.coiled.io/blog/performance-testing.html?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;blog post&lt;/a&gt; on our approach to performance testing, here is a summary:
With &lt;a href="https://github.com/coiled/benchmarks"&gt;&lt;code&gt;coiled/benchmarks&lt;/code&gt;&lt;/a&gt;, we created a benchmark suite that contains a variety of common workloads and operations with Dask, including standardized ones like the &lt;a href="https://github.com/h2oai/db-benchmark"&gt;&lt;code&gt;h2oai/db-benchmark&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It also contains tooling that allows us to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatically &lt;a href="https://blog.coiled.io/blog/performance-testing.html#nightly-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;detect performance regressions&lt;/a&gt; in Dask and raise them as issues.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.coiled.io/blog/performance-testing.html#a-b-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Run A/B tests&lt;/a&gt; to assess the performance impact of different versions of Dask, upstream packages, or cluster configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the former started this journey, the latter will also come in handy soon.  &lt;/p&gt;
&lt;h2 id="identifying-the-problem"&gt;Identifying the problem&lt;/h2&gt;
&lt;p&gt;Our automated regression testing had alerted us that &lt;a href="https://github.com/coiled/benchmarks/blob/895a13db09eb3172155e7b1260a5698f2284f5b7/tests/benchmarks/test_h2o.py#L140-L151"&gt;&lt;code&gt;test_h2o.py::test_q8&lt;/code&gt;&lt;/a&gt; had experienced &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1542560550"&gt;a significant increase&lt;/a&gt; in runtime across all data sizes and file formats. 
From the &lt;a href="https://benchmarks.coiled.io?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;historical report&lt;/a&gt; of our benchmarking suite, we could see that &lt;code&gt;dask/dask&lt;/code&gt; and &lt;code&gt;dask/distributed&lt;/code&gt; were unlikely to be the culprit: 
Nothing had changed on &lt;code&gt;dask/dask&lt;/code&gt; when the performance started to degrade, and there was only one unrelated change on &lt;code&gt;dask/distributed&lt;/code&gt;. 
That left us with the Coiled platform and upstream packages as possible candidates. &lt;/p&gt;
&lt;p&gt;After digging deeper into the cluster data, we noticed that &lt;code&gt;pandas&lt;/code&gt; had been upgraded from &lt;code&gt;1.5.3&lt;/code&gt; to &lt;code&gt;2.0.1&lt;/code&gt;. 
A major upgrade to &lt;code&gt;pandas&lt;/code&gt; at the same time a dataframe-based workload shows degrading performance? That's suspicious! &lt;/p&gt;
&lt;p&gt;To confirm this suspicion, we ran an A/B test based on the current Dask release (&lt;code&gt;2023.4.1&lt;/code&gt; at the time), testing the impact of the pandas upgrade. 
&lt;a href="https://github.com/coiled/benchmarks/actions/runs/4946428740"&gt;The results&lt;/a&gt; were clear: The runtime increased significantly with &lt;code&gt;pandas=2.0.1&lt;/code&gt; (sample).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/ab-test.png"&gt;&lt;/p&gt;
&lt;p&gt;Having shown that &lt;code&gt;pandas&lt;/code&gt; caused for the performance degradation and that we could reproduce it with the current Dask release, our release process for &lt;code&gt;2023.5.0&lt;/code&gt; &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1543878628"&gt;was cleared&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To further analyze the problem, we also derived a &lt;a href="https://matthewrocklin.com/minimal-bug-reports.html#minimal-complete-verifiable-examples"&gt;minimal local reproducer&lt;/a&gt; from the original workload:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from dask.distributed import Client

client = Client()
uri = &amp;quot;s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet&amp;quot;
ddf = dd.read_parquet(uri, engine=&amp;quot;pyarrow&amp;quot;, storage_options={&amp;quot;anon&amp;quot;: True}).persist()
wait(ddf)

ddf = ddf[[&amp;quot;id6&amp;quot;, &amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;, &amp;quot;v3&amp;quot;]]
(
    ddf[~ddf[&amp;quot;v3&amp;quot;].isna()][[&amp;quot;id6&amp;quot;, &amp;quot;v3&amp;quot;]]
    .groupby(&amp;quot;id6&amp;quot;, dropna=False, observed=True)
    .apply(
        lambda x: x.nlargest(2, columns=&amp;quot;v3&amp;quot;),
        meta={&amp;quot;id6&amp;quot;: &amp;quot;Int64&amp;quot;, &amp;quot;v3&amp;quot;: &amp;quot;float64&amp;quot;},
    )[[&amp;quot;v3&amp;quot;]]
).compute()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="investigating-the-pandas-performance-degradation"&gt;Investigating the pandas performance degradation&lt;/h2&gt;
&lt;p&gt;The only user-visible thing that changed between pandas 1.5.3 and pandas 2.0.1 was the default value
of &lt;code&gt;group_keys&lt;/code&gt; in &lt;code&gt;GroupBy&lt;/code&gt;. Switching to &lt;code&gt;group_keys=False&lt;/code&gt; with version 2.0.1
got us back to the initial runtime.
Now that we knew that pandas was to blame for the performance degradation, we had to create a 
reproducer in plain pandas to help fix the issue.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame(
    {
        &amp;quot;foo&amp;quot;: np.random.randint(1, 50_000, (100_000, )), 
        &amp;quot;bar&amp;quot;: np.random.randint(1, 100_000, (100_000, )),
    },
)

df.groupby(
    &amp;quot;foo&amp;quot;, group_keys=False
).apply(lambda x: x.nlargest(2, columns=&amp;quot;bar&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;group_keys=False&lt;/code&gt;: approx. 11 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;group_keys=True&lt;/code&gt;: approx. 15 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experimenting a bit showed us that the bottleneck got even worse while increasing the number of 
groups during the &lt;code&gt;groupby&lt;/code&gt; calculation. We settled on this version which is 30% slower with 
&lt;code&gt;group_keys=True&lt;/code&gt;, enough to be able to troubleshoot the problem. There was no obvious reason
why the changed value should bring a significant slowdown.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%prun&lt;/code&gt; showed us that the time was almost exclusively spent in a post-processing step that
combines all groups via &lt;code&gt;concat&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="addressing-the-performance-degradation"&gt;Addressing the performance degradation&lt;/h2&gt;
&lt;p&gt;Let's look at how both cases differ. The new version passes the grouping levels to &lt;code&gt;concat&lt;/code&gt;, which
are used to construct the resulting Index levels. This shouldn't be that slow though. Investigations
showed that this runs through a code-path that is very inefficient!&lt;/p&gt;
&lt;p&gt;Looking closer at the results of &lt;code&gt;%prun&lt;/code&gt; pointed us to one specific loop that took up most of
the runtime. This loop calculates the &lt;code&gt;codes&lt;/code&gt; for the resulting index based on the provided 
levels. It's slow, really slow! Every single element provided as &lt;code&gt;keys&lt;/code&gt;, which 
represent the number of groups, is checked against the whole level, which explains our previous 
observation that the runtime got worse with an increasing number of groups. You can check out the 
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#more-concatenating-with-group-keys"&gt;pandas user guide&lt;/a&gt;
if you are interested in situations where this is useful. Fortunately, we have a convenient
advantage in case of &lt;code&gt;groupby&lt;/code&gt;. We know beforehand that every key equals the specific level. 
We added a fast-path that exploits this knowledge getting the runtime of this step more or less to
zero.&lt;/p&gt;
&lt;p&gt;This change resulted in a &lt;a href="https://github.com/pandas-dev/pandas/pull/53195"&gt;small PR&lt;/a&gt; that cut 
the runtime of &lt;code&gt;group_keys=True&lt;/code&gt; to approximately 11 seconds as well.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Now that we made our pandas reproducer run 30% faster, we have to check whether we accomplished our
initial objective. Re-running the local Dask reproducer should give us an idea about the
impact on Dask. We got the performance down to 22 seconds as well! Promising news that saved
our plans for the evening!&lt;/p&gt;
&lt;p&gt;Unfortunately, we had to wait until pandas 2.0.2 was released to run a proper benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/benchmark_after.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This looks great! Our small pandas change translated to our Dask query and got performance back
to the previous level!&lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="performance"></category><category term="coiled"></category><category term="pandas"></category></entry><entry><title>Benchmarking pandas against Polars from a pandas PoV</title><link href="https://phofl.github.io/pandas-benchmarks.html" rel="alternate"></link><published>2023-06-15T00:00:00+02:00</published><updated>2023-06-15T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-15:/pandas-benchmarks.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Or: How writing efficient pandas code matters&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I've regularly seen benchmarks that show how much faster Polars is compared to pandas. 
The fact that Polars is faster than pandas is not too surprising since it is multithreaded while 
pandas is mostly single-core. The big difference surprises me though. That's …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Or: How writing efficient pandas code matters&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I've regularly seen benchmarks that show how much faster Polars is compared to pandas. 
The fact that Polars is faster than pandas is not too surprising since it is multithreaded while 
pandas is mostly single-core. The big difference surprises me though. That's why I decided to take 
a look at the pandas queries that were used for the benchmarks. &lt;/p&gt;
&lt;p&gt;I was curious to find out whether there was room for improvement. This post will detail a couple 
of easy steps that I took to speed up pandas code. The pandas performance improvements are quite 
impressive!&lt;/p&gt;
&lt;p&gt;We will look at the &lt;code&gt;tpch&lt;/code&gt; benchmarks from
the &lt;a href="https://github.com/pola-rs/tpch"&gt;Polars repository&lt;/a&gt; with &lt;code&gt;scale_1&lt;/code&gt; including I/O time. &lt;/p&gt;
&lt;p&gt;Fair warning: I had to try this a couple of times since the API changed. You can switch to version 
0.17.15, if you encounter problems, that's what I used. Additionally, I am using the current 
development version of pandas because some optimizations for Copy-on-Write and the 
Pyarrow &lt;code&gt;dtype_backend&lt;/code&gt; were added after 2.0 was released. The development version was also used 
to create the baseline plots, so all performance gains shown in here can be attributed to the 
refactoring steps. You can use this version starting from August at the latest!&lt;/p&gt;
&lt;p&gt;I came away with one main takeaway:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing efficient pandas code &lt;strong&gt;matters a lot&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have a MacBook Air with M2 processors and 24GB of RAM. The benchmarks are only run once in 
default mode. I repeated the calculations 15 times and used the mean as result.&lt;/p&gt;
&lt;h2 id="baseline"&gt;Baseline&lt;/h2&gt;
&lt;p&gt;I ran the benchmarks "as is" as a first step to get the status-quo.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/baseline.png"&gt;&lt;/p&gt;
&lt;p&gt;It's relatively easy to see that Polars is between 4–10 times faster than pandas. After getting 
these results I decided to look at the queries that were used for pandas. A couple of relatively 
straightforward optimizations will speed up our pandas code a lot. Additionally, we will get other
benefits out of it as well, like a significantly reduced memory footprint.&lt;/p&gt;
&lt;p&gt;Side note: Number 8 is broken, so no result there.&lt;/p&gt;
&lt;h2 id="initial-refactoring"&gt;Initial refactoring&lt;/h2&gt;
&lt;p&gt;One thing that stood out is that the whole parquet files were read even though most queries only 
needed a small subset. Some queries also did some operations on the whole dataset and dropped the 
columns later on. A filtering operation is slowed down quite a bit when performed on the full 
DataFrame compared to only a fraction of the columns, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;new_df = df[mask]
new_df = df[[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is significantly slower than restricting the DataFrame to the relevant subset beforehand:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df[[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;]]
new_df = df[mask]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no need to read these columns at all, if they are not used somewhere within your code. 
Pushing the column selection into &lt;code&gt;read_parquet&lt;/code&gt; is easy, since this is offered by PyArrow through 
the &lt;code&gt;columns&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.read_parquet(...)
df = df[df.a &amp;gt; 100]
df[[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is rewritten into:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.read_parquet(..., columns=[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;])
df = df[df.a &amp;gt; 100]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I've also turned Copy-on-Write on. It's now in a state that it shouldn't have many performance 
problems while it will most likely give a speedup. That said, the difference here is not too big, 
since the benchmarks are &lt;code&gt;GroupBy&lt;/code&gt; and &lt;code&gt;merge&lt;/code&gt; heavy, which aren't really influenced by CoW. 
This was a quick refactoring effort that took me around 30 minutes for all queries. Most queries 
restricted the DataFrame later on anyway, so it was mostly a copy-paste exercise.&lt;/p&gt;
&lt;p&gt;Let's look at the results:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/first_optimization.png"&gt;&lt;/p&gt;
&lt;p&gt;The pandas queries got a lot faster through a couple of small modifications, e.g. we can see 
performance improvements by a factor of 2 and more. Since this avoids loading unnecessary columns 
completely, we reduced the memory footprint of our program significantly.&lt;/p&gt;
&lt;h2 id="further-optimizations-leveraging-arrow"&gt;Further optimizations - leveraging Arrow&lt;/h2&gt;
&lt;p&gt;A quick profiling showed that the filter operations were still a bottleneck for a couple of queries. 
Fortunately, there is an easy fix. &lt;code&gt;read_parquet&lt;/code&gt; passes potential keywords through to PyArrow and 
Arrow offers the option to filter the table while reading the parquet file. Moving these filters up 
gives a nice additional improvement.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.read_parquet(..., columns=[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;])
df = df[df.a &amp;gt; 100]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily pass the filter condition to Arrow to avoid materializing unnecessary rows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pyarrow.compute as pc


df = pd.read_parquet(..., columns=[&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;], filters=pc.field(&amp;quot;a&amp;quot;) &amp;gt; 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrow supports an Expression style for these filter conditions. You can use more or less all 
filtering operations that would be available in pandas as well. Let's look at what this means 
performance-wise.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/second_optimization.png"&gt;&lt;/p&gt;
&lt;p&gt;We got a nice performance improvement for most queries. Obviously, this depends on what filter was 
used in the query itself. Similar to the first optimization, this will reduce the memory footprint 
significantly, since rows violating the filter won't be loaded into memory at all.&lt;/p&gt;
&lt;p&gt;There is one relatively straightforward optimization left without rewriting the queries completely.&lt;/p&gt;
&lt;h2 id="improving-merge-performance"&gt;Improving &lt;code&gt;merge&lt;/code&gt; performance&lt;/h2&gt;
&lt;p&gt;The following trick requires knowledge about your data and can’t be applied in a general way. This 
is more of a general idea that isn’t really compliant with tpch rules, but nice to see nevertheless.&lt;/p&gt;
&lt;p&gt;This technique is a bit tricky. I stumbled upon this a couple of years ago when I had a performance
issue at my previous job. In these scenarios, &lt;code&gt;merge&lt;/code&gt; is basically used as a filter that restricts
one of both DataFrames quite heavily. This is relatively slow when using merge, because pandas
isn't aware that you want to use the &lt;code&gt;merge&lt;/code&gt; operation as a filter. We can apply a filter
with isin before performing the actual merge to speed up our queries.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

left = pd.DataFrame({&amp;quot;left_a&amp;quot;: [1, 2, 3], &amp;quot;left_b&amp;quot;: [4, 5, 6]})
right = pd.DataFrame({&amp;quot;right_a&amp;quot;: [1], &amp;quot;right_c&amp;quot;: [4]})

left = left[left[&amp;quot;left_a&amp;quot;].isin(right[&amp;quot;right_a&amp;quot;])]  # restrict the df beforehand
result = left.merge(right, left_on=&amp;quot;left_a&amp;quot;, right_on=&amp;quot;right_a&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This wasn't necessary for all queries, since only a couple of them were using &lt;code&gt;merge&lt;/code&gt; in this way. 
The queries where we applied the technique got another nice performance boost!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/pandas_benchmark/final_optimization.png"&gt;&lt;/p&gt;
&lt;p&gt;We achieved our main objective here. Most of these queries run a lot faster now than they did 
before, and use less memory as well. The first query did not get that big of a speedup. It 
calculates a lot of results on a groupby aggregation, which is done sequentially. We are looking
into it right now how we can improve performance here.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;All in all these optimizations took me around 1.5-2 hours. Your mileage might vary, but I don't 
think that it will take you much longer, since most of the optimizations are very straightforward. 
A relatively small time investment where most
of the time was spent on reorganizing the initial queries. You can find the PR that modifies the 
benchmarks &lt;a href="https://github.com/pola-rs/tpch/pull/56"&gt;here&lt;/a&gt;. Polars has a query optimization layer, so it does some of
these things automatically, but this is not a guarantee that you'll end up with efficient code. &lt;/p&gt;
&lt;p&gt;We saw that writing more efficient pandas code isn't that hard and can give you a huge performance 
improvement. Looking at these queries helped us as well, since we were able to identify a couple of 
performance bottleneck where we are working on a solution. We were even able to make one of the 
queries run faster than the Polars version!&lt;/p&gt;
&lt;p&gt;These performance improvements mostly translate to the &lt;code&gt;scale_10&lt;/code&gt; version of the benchmarks.&lt;/p&gt;
&lt;p&gt;Thanks for reading. Please reach out with any comments or feedback. I wrote a more general post about
&lt;a href="https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b"&gt;improving performance in pandas with PyArrow&lt;/a&gt; 
as well.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category></entry><entry><title>Utilizing PyArrow to improve pandas and Dask workflows</title><link href="https://phofl.github.io/pyarrow-pandas-dask.html" rel="alternate"></link><published>2023-06-05T00:00:00+02:00</published><updated>2023-06-05T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-05:/pyarrow-pandas-dask.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/title.svg"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/title.svg"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch of long-standing pains for users of both libraries. pandas users often complain to me that
pandas does not support missing values in arbitrary dtypes or that non-standard dtypes are not very
well supported. A particularly annoying problem for
Dask users is running out of memory with large datasets. PyArrow backed string columns 
consume up to 70% less memory compared to NumPy object columns and thus have the potential to 
mitigate this problem as well as providing a huge performance improvement.&lt;/p&gt;
&lt;p&gt;Support for PyArrow dtypes in pandas, and by extension Dask, is still relatively new. I would 
recommend caution when opting into the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; until at least pandas 2.1 is 
released. Not every part of both APIs is optimized yet. You should be able to get a big improvement 
in certain workflows though. This post will go over a couple of examples where I'd recommend switching to 
PyArrow right away, because it already provides huge benefits. &lt;/p&gt;
&lt;p&gt;Dask itself can benefit in various ways from PyArrow dtypes. We will investigate how PyArrow backed
strings can easily mitigate the pain point of running out of memory on Dask clusters and how we 
can improve performance through utilizing PyArrow.&lt;/p&gt;
&lt;p&gt;I am part of the pandas core team and was heavily involved in implementing and improving PyArrow 
support in pandas. I've recently joined 
&lt;a href="https://www.coiled.io?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; where I 
am working on Dask. One of my tasks is improving the PyArrow integration.&lt;/p&gt;
&lt;h2 id="general-overview-of-pyarrow-support"&gt;General overview of PyArrow support&lt;/h2&gt;
&lt;p&gt;PyArrow dtypes were initially introduced in pandas 1.5. The implementation was experimental and I
wouldn't recommend using it on pandas 1.5.x. Support for them is still relatively new. 
pandas 2.0 provides a huge improvement, including making opting into PyArrow backed DataFrames easy.
We are still working on supporting them properly everywhere, and thus they should be used with caution
until at least pandas 2.1 is released. Both projects work continuously to improve support throughout Dask and 
pandas.&lt;/p&gt;
&lt;p&gt;We encourage users to try them out! This will help us to get a better idea of what is still lacking
support or is not fast enough. Giving feedback helps us improve support and will drastically reduce the
time that is necessary to create a smooth user experience.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We will use the taxi dataset from New York City that contains all Uber and Lyft rides. It has
some interesting attributes like price, tips, driver pay and many more. The dataset can be found
&lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;here&lt;/a&gt; 
(see &lt;a href="https://www.nyc.gov/home/terms-of-use.page"&gt;terms of service&lt;/a&gt;) and is stored in parquet
files. When analyzing Dask queries, we will use a publicly available S3 bucket to simplify our
queries: &lt;code&gt;s3://coiled-datasets/uber-lyft-tlc/&lt;/code&gt;. We will use the dataset from December 2022 
for our pandas queries, since this is the maximum that fits comfortably into memory on my 
machine (24GB of RAM). We have to avoid stressing our RAM usage, since this might introduce side 
effects when analyzing performance.&lt;/p&gt;
&lt;p&gt;We will also investigate the performance of &lt;code&gt;read_csv&lt;/code&gt;. We will use the &lt;em&gt;Crimes in Chicago&lt;/em&gt; dataset
that can be found &lt;a href="https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="dask-cluster"&gt;Dask cluster&lt;/h2&gt;
&lt;p&gt;There are various different options to set up a Dask cluster, see the 
&lt;a href="https://docs.dask.org/en/stable/deploying.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask documentation&lt;/a&gt; for
a non-exhaustive list of deployment options. I will use
&lt;a href="https://docs.coiled.io/user_guide/index.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; to create a cluster on AWS with
30 machines through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

cluster = coiled.Cluster(
    n_workers=30,
    name=&amp;quot;dask-performance-comparisons&amp;quot;,
    region=&amp;quot;us-east-2&amp;quot;,  # this is the region of our dataset
    worker_vm_type=&amp;quot;m6i.large&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled is connected to my AWS account. It creates the cluster within my account and manages all
resources for me. 30 machines are enough to operate on our dataset comfortably. We will investigate
how we can reduce the required number of workers to 15 through some small 
modifications.&lt;/p&gt;
&lt;h2 id="pandas-stringdtype-backed-by-pyarrow"&gt;pandas StringDtype backed by PyArrow&lt;/h2&gt;
&lt;p&gt;We begin with a feature that was originally introduced over 3 years ago in pandas 1.0. Setting the
dtype in pandas or Dask to &lt;code&gt;string&lt;/code&gt; returns an object with &lt;code&gt;StringDtype&lt;/code&gt;. This feature is relatively mature and should
provide a smooth user experience.&lt;/p&gt;
&lt;p&gt;Historically, pandas represented string data through NumPy arrays with dtype &lt;code&gt;object&lt;/code&gt;. NumPy object data is stored as an 
array of pointers pointing to the actual data in memory. This makes iterating over an array containing 
strings very slow. pandas 1.0 initially introduced said
&lt;code&gt;StringDtype&lt;/code&gt; that allowed easier and consistent operations on strings. This dtype was still backed by Python 
strings and thus, wasn't very performant either. Rather, it provided a clear abstraction of string
data.&lt;/p&gt;
&lt;p&gt;pandas 1.3 finally introduced an enhancement to create an efficient string dtype. This datatype is backed by PyArrow arrays.
&lt;a href="https://arrow.apache.org/docs/python/index.html"&gt;PyArrow&lt;/a&gt; provides a data structure that enables 
performant and memory efficient string operations.
Starting from that point on, users could use a string dtype that was contiguous in memory and thus
very fast. This dtype can be requested through &lt;code&gt;string[pyarrow]&lt;/code&gt;. Alternatively, we can request it
by specifying &lt;code&gt;string&lt;/code&gt; as the dtype and setting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Dask builds on top of pandas, this string dtype is available here as well. On top of that, 
Dask offers a convenient option that automatically converts all string-data to &lt;code&gt;string[pyarrow]&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a convenient way of
avoiding NumPy object dtype for string columns. Additionally, it has the advantage that it
creates PyArrow arrays natively for I/O methods that operate with Arrow objects. 
On top of providing huge performance improvements, PyArrow strings consume significantly less
memory. An average Dask DataFrame with PyArrow strings consumes around 33-50% of the original
memory compared to NumPy object. This solves the biggest pain point for Dask users that is running
out of memory when operating on large datasets. The option enables global testing in Dask's test
suite. This ensures that PyArrow backed strings are mature enough to provide a smooth user
experience.&lt;/p&gt;
&lt;p&gt;Let's look at a few operations that represent typical string operations. We will start with a couple
of pandas examples before switching over to operations on our Dask cluster.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;df.convert_dtypes&lt;/code&gt; to convert our object columns to PyArrow string arrays. There
are more efficient ways of getting PyArrow dtypes in pandas that we will explore later. We will use
the Uber-Lyft dataset from December 2022, this file fits comfortably into memory on my machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;

df = pd.read_parquet(
    &amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;,
    columns=[
        &amp;quot;tips&amp;quot;, 
        &amp;quot;hvfhs_license_num&amp;quot;, 
        &amp;quot;driver_pay&amp;quot;, 
        &amp;quot;base_passenger_fare&amp;quot;, 
        &amp;quot;dispatching_base_num&amp;quot;,
    ],
)
df = df.convert_dtypes(
    convert_boolean=False, 
    convert_floating=False, 
    convert_integer=False,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our DataFrame has NumPy dtypes for all non-string columns in this example. Let's start with
filtering for all rides that were operated by Uber.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation creates a mask with True/False values that specify whether Uber operated a ride. 
This doesn't utilize any special string methods, but the equality comparison dispatches to 
PyArrow. Next, we will use the String accessor that is implemented in pandas and gives you access
to all kinds of string operations on a per-element basis. We want to find all rides that were
dispatched from a base starting with &lt;code&gt;"B028"&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;startswith&lt;/code&gt; iterates over our array and checks whether every string starts with the specified
substring. The advantage of PyArrow is easy to see. The data are contiguous in memory, which means
that we can efficiently iterate over them. Additionally, these arrays have a second array with 
pointers that point to the first memory address of every string, which makes computing the starting
sequence even faster.&lt;/p&gt;
&lt;p&gt;Finally, we look at a &lt;code&gt;GroupBy&lt;/code&gt; operation that groups over PyArrow string columns. The calculation
of the groups can dispatch to PyArrow as well, which is more efficient than factorizing
over a NumPy object array.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's look at how these operations stack up against DataFrames where string columns are represented
by NumPy object dtype.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/pandas_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;The results are more or less as we expected. The string based comparisons are significantly faster
when performed on PyArrow strings. Most string accessors should provide a huge performance 
improvement. Another interesting observation is memory usage, it is reduced by roughly 50% compared
to NumPy object dtype. We will take a closer look at this with Dask.&lt;/p&gt;
&lt;p&gt;Dask mirrors the pandas API and dispatches to pandas for most operations. Consequently, we can use
the same API to access PyArrow strings. A convenient option to request these globally is the option
mentioned above, which is what we will use here:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the biggest benefits of this option during development is that it enables easy testing of PyArrow
strings globally in Dask to make sure that everything works smoothly. We will utilize the Uber-Lyft
dataset for our explorations. The dataset takes up around 240GB of memory on our cluster. Our initial
cluster has 30 machines, which is enough to perform our computations comfortably.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask
import dask.dataframe as dd
from distributed import wait

dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)  # Wait till the computation is finished
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We persist the data in memory so that I/O performance does not influence our performance measurements. Our data is now
available in memory, which makes access fast. We will perform computations that are similar to our
pandas computations. One of the main goals is to show that the benefits from pandas will 
translate to computations in a distributed environment with Dask.&lt;/p&gt;
&lt;p&gt;One of the first observations is that the DataFrame with PyArrow backed string columns consumes only
130GB of memory, only half of what it consumed with NumPy object columns. We have only a few string
columns in our DataFrame, which means that the memory savings for string columns are actually higher than around 50%
when switching to PyArrow strings. Consequently, we will reduce the size of our cluster to 15 workers
when performing our operations on PyArrow string columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cluster.scale(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We measure the performance of the mask-operation and one of the String accessors together through
subsequent filtering of the DataFrame. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
df = df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we can use the same methods as in our previous example. This makes transitioning from
pandas to Dask relatively easy.&lt;/p&gt;
&lt;p&gt;Additionally, we will again compute a &lt;code&gt;GroupBy&lt;/code&gt; operation on our data. This is significantly harder
in a distributed environment, which makes the results more interesting. The previous operations
parallelize relatively easy onto a large cluster, while this is harder with &lt;code&gt;GroupBy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)

df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/Dask_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;We get nice improvements by factors of 2 and 3. This is especially intriguing since we reduced
the size of our cluster from 30 machines to 15, reducing the cost by 50%. Subsequently, we also reduced our computational 
resources by a factor of 2, which makes our performance improvement even more impressive. Thus,
the performance improved by a factor of 4 and 6 respectively. We can
perform the same computations on a smaller cluster, which saves money and is more efficient in general
and still get a performance boost out of it.&lt;/p&gt;
&lt;p&gt;Summarizing, we saw that PyArrow string-columns are a huge improvement to NumPy object columns in
DataFrames. Switching to PyArrow strings is a relatively small change that might improve the 
performance and efficiency of an average workflow that depends on string data. These improvements 
are visible in pandas and Dask!&lt;/p&gt;
&lt;h2 id="engine-keyword-in-io-methods"&gt;Engine keyword in I/O methods&lt;/h2&gt;
&lt;p&gt;We will now take a look at I/O functions in pandas and Dask. Some functions have custom implementations, 
like &lt;code&gt;read_csv&lt;/code&gt;, while others dispatch to another library, like &lt;code&gt;read_excel&lt;/code&gt; to 
&lt;code&gt;openpyxl&lt;/code&gt;. Some of these functions gained a new &lt;code&gt;engine&lt;/code&gt; keyword that enables us to dispatch to 
&lt;code&gt;PyArrow&lt;/code&gt;. The PyArrow parsers are multithreaded by default and hence, can provide a significant 
performance improvement.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This configuration will return the same results as the other engines. The only difference is that
PyArrow is used to read the data. The same option is available for &lt;code&gt;read_json&lt;/code&gt;.
The PyArrow-engines were added to provide a faster way of reading data. The improved speed is only
one of the advantages. The PyArrow parsers return the data as a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.Table.html"&gt;PyArrow Table&lt;/a&gt;. A PyArrow Table
provides built-in functionality to convert to a pandas &lt;code&gt;DataFrame&lt;/code&gt;. Depending on the data, this
might require a copy while casting to NumPy (string, integers with missing values, ...), which
brings an unnecessary slowdown. This is where the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; comes in.
It is implemented as an &lt;code&gt;ArrowExtensionArray&lt;/code&gt; class in pandas, which is backed by a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html"&gt;PyArrow ChunkedArray&lt;/a&gt;.
As a direct consequence, the conversion from a PyArrow Table to pandas is extremely cheap since it
does not require any copies. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;, dtype_backend=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a &lt;code&gt;DataFrame&lt;/code&gt; that is backed by PyArrow arrays. pandas isn't optimized everywhere
yet, so this can give you a slowdown in follow-up operations. It might be worth it if
the workload is particularly I/O heavy. Let's look at a direct comparison:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/pandas_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can see that PyArrow-engine and PyArrow dtypes provide a 15x speedup compared
to the C-engine.&lt;/p&gt;
&lt;p&gt;The same advantages apply to Dask. Dask wraps the pandas csv reader and
hence, gets the same features for free.&lt;/p&gt;
&lt;p&gt;The comparison for Dask is a bit more complicated. Firstly, my example reads the data from my local machine while
our Dask examples will read the data from a S3 bucket. Network speed will
be a relevant component. Also, distributed computations have some
overhead that we have to account for. &lt;/p&gt;
&lt;p&gt;We are purely looking for speed here, so we will read some timeseries data
from a public S3 bucket. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_csv(
    &amp;quot;s3://coiled-datasets/timeseries/20-years/csv/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
    engine=&amp;quot;pyarrow&amp;quot;,
    parse_dates=[&amp;quot;timestamp&amp;quot;],
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will execute this code-snippet for &lt;code&gt;engine="c"&lt;/code&gt;, &lt;code&gt;engine="pyarrow"&lt;/code&gt; and additionally
&lt;code&gt;engine="pyarrow"&lt;/code&gt; with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at some performance comparisons.
Both examples were executed with 30 machines on the cluster.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/Dask_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;The PyArrow-engine runs around 2 times as fast as the C-engine. Both implementations used the same
number of machines. The memory usage was reduced by 50% with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. The same
reduction is available if only object columns are converted to PyArrow strings, which gives
a better experience in follow-up operations.&lt;/p&gt;
&lt;p&gt;We've seen that the Arrow-engines provide significant speedups over the custom C implementations.
They don't support all features of the custom implementations yet, but if your use-case is 
compatible with the supported options, you should get a significant speedup for free.&lt;/p&gt;
&lt;p&gt;The case with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is a bit more complicated. Not all areas of the API are
optimized yet. If you spend a lot of time processing your data outside I/O functions, then this might not 
give you what you need. It will speed up your processing if your workflow spends a lot of
time reading the data.&lt;/p&gt;
&lt;h2 id="dtype_backend-in-pyarrow-native-io-readers"&gt;dtype_backend in PyArrow-native I/O readers&lt;/h2&gt;
&lt;p&gt;Some other I/O methods have an engine keyword as well. &lt;code&gt;read_parquet&lt;/code&gt; is the most popular 
example. The situation is a bit different here though. These I/O methods were already using the
PyArrow engine by default. So the parsing is as efficient as possible. One other potential
performance benefit is the usage of the &lt;code&gt;dtype_backend&lt;/code&gt; keyword. Normally, PyArrow will return
a PyArrow table which is then converted to a pandas DataFrame. The PyArrow dtypes are converted to
their NumPy equivalent. Setting &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt; avoids this conversion. This gives 
a decent performance improvement and saves a lot of memory.&lt;/p&gt;
&lt;p&gt;Let's look at one pandas performance comparison. We read the Uber-Lyft taxi data from December 2022.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_parquet(&amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the data with and without &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/pandas_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can easily see that the most time is taken up by the conversion after the reading of the
Parquet file was finished. The function runs 3 times as fast when avoiding
the conversion to NumPy dtypes.&lt;/p&gt;
&lt;p&gt;Dask has a specialized implementation for &lt;code&gt;read_parquet&lt;/code&gt; that has some advantages tailored to
distributed workloads compared to the pandas implementation. The common denominator is that both
functions dispatch to PyArrow to read the parquet file. Both have in common that the data are
converted to NumPy dtypes after successfully reading the file. We are reading
the whole Uber-Lyft dataset, which consumes around 240GB of memory on our
cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the dataset in 3 different configurations. First with the default NumPy dtypes, then with
the PyArrow string option turned on:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at what this means performance-wise:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/Dask_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;Similar to our pandas example, we can see that converting to NumPy dtypes takes up a huge chunk of
our runtime. The PyArrow dtypes give us a nice performance improvement. Both PyArrow configurations
use half of the memory that the NumPy dtypes are using.&lt;/p&gt;
&lt;p&gt;PyArrow-strings are a lot more mature than the general PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. Based on the 
performance chart we got, we get roughly the same performance improvement when using PyArrow 
strings and NumPy dtypes for all other dtypes. If a workflow does not work well enough on PyArrow 
dtypes yet, I'd recommend enabling PyArrow strings only.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen how we can leverage PyArrow in pandas in Dask right now. PyArrow backed string columns have the
potential to impact most workflows in a positive way and provide a smooth user experience with
pandas 2.0. Dask has a convenient option to globally avoid NumPy object dtype when possible, which
makes opting into PyArrow backed strings even easier. PyArrow also provides huge speedups in other
areas where available. The PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is still pretty new and has the 
potential to cut I/O times significantly right now. It is certainly worth exploring whether it can solve
performance bottlenecks. There is a lot of work going on to improve support for general PyArrow
dtypes with the potential to speed up an average workflow in the near future.&lt;/p&gt;
&lt;p&gt;There is a current proposal in pandas to start inferring strings as PyArrow backed strings by
default starting from pandas 3.0. Additionally, it includes many more areas where leaning more
onto PyArrow makes a lot of sense (e.g. Decimals, structured data, ...). You can read up on the
proposal &lt;a href="https://github.com/pandas-dev/pandas/pull/52711"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out to share your thoughts and feedback 
about PyArrow support in both libraries.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category><category term="dask"></category></entry><entry><title>Welcoming pandas 2.0</title><link href="https://phofl.github.io/pandas-20.html" rel="alternate"></link><published>2023-03-23T00:00:00+01:00</published><updated>2023-03-23T00:00:00+01:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-03-23:/pandas-20.html</id><summary type="html">&lt;p&gt;&lt;em&gt;How the API is changing and how to leverage new functionalities&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After 3 years of development, the second pandas 2.0 release candidate was released on the 16th of 
March. There are many new features in pandas 2.0, including improved extension array
support, pyarrow support for DataFrames and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;How the API is changing and how to leverage new functionalities&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After 3 years of development, the second pandas 2.0 release candidate was released on the 16th of 
March. There are many new features in pandas 2.0, including improved extension array
support, pyarrow support for DataFrames and non-nanosecond datetime resolution, but also
many enforced deprecations and hence API changes. Before we investigate how new features can improve
your workflow, we take a look at some enforced deprecations.&lt;/p&gt;
&lt;h2 id="api-changes"&gt;API changes&lt;/h2&gt;
&lt;p&gt;The 2.0 release is a major release for pandas (check out the 
&lt;a href="https://pandas.pydata.org/docs/development/policies.html#version-policy"&gt;versioning policy&lt;/a&gt;), 
hence all deprecations added in the 1.x series were enforced.
There were around 150 different warnings in the latest 1.5.3 release. If your code runs without
warnings on 1.5.3, you should be good to go on 2.0. We will have a quick look at some
subtle or more noticeable deprecations before jumping into new features. You can check out the
complete release notes &lt;a href="https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="index-now-supports-arbitrary-numpy-dtypes"&gt;Index now supports arbitrary NumPy dtypes&lt;/h3&gt;
&lt;p&gt;Before the 2.0 release, an &lt;code&gt;Index&lt;/code&gt; only supported &lt;code&gt;int64&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt; and &lt;code&gt;uint64&lt;/code&gt; dtypes 
which resulted in an &lt;code&gt;Int64Index&lt;/code&gt;, &lt;code&gt;Float64Index&lt;/code&gt; or &lt;code&gt;UInt64Index&lt;/code&gt;. These classes where 
removed. All numeric indexes are now represented as &lt;code&gt;Index&lt;/code&gt; with an associated dtype, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;In [1]: pd.Index([1, 2, 3], dtype=&amp;quot;int64&amp;quot;)
Out[1]: Index([1, 2, 3], dtype='int64')
In [2]: pd.Index([1, 2, 3], dtype=&amp;quot;int32&amp;quot;)
Out[2]: Index([1, 2, 3], dtype='int32')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This mirrors the behavior for extension array backed Indexes. An Index can hold arbitrary extension 
array dtypes since pandas 1.4.0. You can check the 
&lt;a href="https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#index-can-now-hold-numpy-numeric-dtypes"&gt;release notes&lt;/a&gt; 
for further information. This change is only noticeable when an explicit Index subclass, that no
longer exists, is used.&lt;/p&gt;
&lt;h3 id="behavior-change-in-numeric_only-for-aggregation-functions"&gt;Behavior change in &lt;code&gt;numeric_only&lt;/code&gt; for aggregation functions&lt;/h3&gt;
&lt;p&gt;In previous versions you could call aggregation functions on a DataFrame with mixed-dtypes and
got varying results. Sometimes the aggregation worked and excluded non-numeric dtypes, in some
other cases an error was raised. The &lt;code&gt;numeric_only&lt;/code&gt; argument is now consistent and the aggregation
will raise if you apply it on a DataFrame with non-numeric dtypes. You can set &lt;code&gt;numeric_only&lt;/code&gt;
to &lt;code&gt;True&lt;/code&gt; or restrict your DataFrame to numeric columns, if you want to get the same behavior
as before. This will avoid accidentally dropping relevant columns from the &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Calculating the mean over a DataFrame dropped non-numeric columns before 2.0:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;In[2] df = DataFrame({&amp;quot;a&amp;quot;: [1, 2, 3], &amp;quot;b&amp;quot;: [&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;z&amp;quot;]})
In[3] df.mean()
Out[3]: 
a    2.0
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation now raises an error to avoid dropping relevant columns in these aggregations:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;TypeError: Could not convert ['xyz'] to numeric
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="improvements-and-new-features"&gt;Improvements and new features&lt;/h2&gt;
&lt;p&gt;pandas 2.0 brings a some interesting new functionalities like PyArrow-backed DataFrames, 
non-nanosecond resolution/accuracy for timestamps and many Copy-on-Write improvements. Let's take
a closer look at some of those now.&lt;/p&gt;
&lt;h3 id="improved-support-for-nullable-dtypes-and-extension-arrays"&gt;Improved support for nullable dtypes and extension arrays&lt;/h3&gt;
&lt;p&gt;The 2.0 release brings a vast improvement for nullable dtypes and extension arrays in general.
Internally, many operations now use nullable semantics instead of casting to object when
using nullable dtypes like &lt;code&gt;Int64&lt;/code&gt;, &lt;code&gt;boolean&lt;/code&gt; or &lt;code&gt;Float64&lt;/code&gt;. The internal handling of extension
arrays got consistently better over the 1.x series. This is visible through
a bunch of significant performance improvements:&lt;/p&gt;
&lt;p&gt;On pandas 2.0:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;In[3]: ser = pd.Series(list(range(1, 1_000_000)) + [pd.NA], dtype=&amp;quot;Int64&amp;quot;)
In[4]: %timeit ser.drop_duplicates()
7.54 ms ± 24 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On pandas 1.5.3:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;In[3]: ser = pd.Series(list(range(1, 1_000_000)) + [pd.NA], dtype=&amp;quot;Int64&amp;quot;)
In[4]: %timeit ser.drop_duplicates()
22.7 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, many operations now properly operate on the nullable arrays which maintains the
appropriate dtype when returning the result. All groupby algorithms now use nullable semantics,
which results in better accuracy (previously the input was cast to float which might have let
to a loss of precision) and performance improvements.&lt;/p&gt;
&lt;p&gt;To improve opting into nullable dtypes, a new keyword &lt;code&gt;dtype_backen&lt;/code&gt; which returns
a &lt;code&gt;DataFrame&lt;/code&gt; completely backed by nullable dtypes when set to &lt;code&gt;"numpy_nullable"&lt;/code&gt; was added to 
most I/O functions. In addition to using nullable dtypes for numeric columns, 
this option results in a &lt;code&gt;DataFrame&lt;/code&gt; that uses the pandas &lt;code&gt;StringDtype&lt;/code&gt;
instead of a NumPy array with dtype &lt;code&gt;object&lt;/code&gt;. Based on the storage option, the string columns
are either backed by Python strings or by PyArrow strings. The PyArrow alternative is generally
faster than the Python strings.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Index&lt;/code&gt; and &lt;code&gt;MultiIndex&lt;/code&gt; classes are now better integrated with extension
arrays in general. General Extension Array support was introduced in 1.4. A quick overview of what
this entails:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using extension array semantics for operations on the index&lt;/li&gt;
&lt;li&gt;Efficient Indexing operations on nullable and pyarrow dtypes&lt;/li&gt;
&lt;li&gt;No materialization of MultiIndexes to improve performance and maintain dtypes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Extension Array interface is continuously improving and continues to avoid materializing NumPy 
arrays and instead relies on the provided extension array implementation in a growing number
of methods. Some areas are still under development, including GroupBy aggregations for third party 
extension arrays.&lt;/p&gt;
&lt;h3 id="pyarrow-backed-dataframes"&gt;Pyarrow-backed DataFrames&lt;/h3&gt;
&lt;p&gt;Version 1.5.0 brought a new extension array to pandas that enables users to create &lt;code&gt;DataFrames&lt;/code&gt;
backed by PyArrow arrays. We expect these extension arrays to provide a vast improvement when
operating on string-columns, since the NumPy object representation is not very efficient. The
string representation is mostly equivalent to&lt;code&gt;string[pyarrow]&lt;/code&gt; that has been around for quite some 
time. The PyArrow-specific extension array supports all other PyArrow dtypes on top of it. Users can 
now create columns with any PyArrow dtype and/or use PyArrow nullable semantics. Those
come out of the box when using PyArrow dtypes. A PyArrow-backed column can be requested 
specifically by casting to or specifying a column's dtype as &lt;code&gt;f"{dtype}[pyarrow]"&lt;/code&gt;, e.g. 
&lt;code&gt;"int64[pyarrow]"&lt;/code&gt; for an integer column. Alternatively, a PyArrow dtype can be created through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import pyarrow as pa

dtype = pd.ArrowDtype(pa.int64)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The API in 1.5.0 was pretty raw and experimental and fell back to NumPy quite often. With pandas 2.0 and an 
increased minimum version of PyArrow (7.0 for pandas 2.0), we can now utilize the corresponding PyArrow compute 
functions in many more methods. This improves performance significantly and gets rid of many
&lt;code&gt;PerformanceWarnings&lt;/code&gt; that were raised before when falling back to NumPy. Similarly to the
nullable dtypes, most I/O methods can return PyArrow-backed DataFrames through the keyword
&lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Future versions of pandas will bring many more improvements in this area!&lt;/p&gt;
&lt;p&gt;Some I/O methods have specific PyArrow engines, like &lt;code&gt;read_csv&lt;/code&gt; and &lt;code&gt;read_json&lt;/code&gt;, which bring
a significant performance improvement when requesting PyArrow-backed &lt;code&gt;DataFrames&lt;/code&gt;. They don't 
support all options that the original implementations support yet. Check out a more &lt;a href="https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i"&gt;in-depth
exploration&lt;/a&gt; from Marc 
Garcia.&lt;/p&gt;
&lt;h3 id="non-nanosecond-resolution-in-timestamps"&gt;Non-nanosecond resolution in Timestamps&lt;/h3&gt;
&lt;p&gt;A long-standing issue in pandas was that timestamps were always represented in nanosecond 
resolution. As a consequence, there was no way of representing dates before the 1st of January
1970 or after the 11th of April 2264. This caused pains in the research community when analyzing
timeseries data that spanned over millennia and more.&lt;/p&gt;
&lt;p&gt;The 2.0 release introduces support for other resolutions, e.g. support for second, millisecond
and microsecond resolution was added. This enables time ranges up to &lt;code&gt;+/- 2.9e11 years&lt;/code&gt; and thus 
should cover most common use-cases.&lt;/p&gt;
&lt;p&gt;On previous versions, passing a date to the &lt;code&gt;Timestamp&lt;/code&gt; constructor that was out of the supported
range raised an error no matter what unit was specified. With pandas 2.0 the unit is honored, and
thus you can create arbitrary dates:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;In[5]: pd.Timestamp(&amp;quot;1000-10-11&amp;quot;, unit=&amp;quot;s&amp;quot;)
Out[5]: Timestamp('1000-10-11 00:00:00')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The timestamp is only returned up to the second, higher precisions are not supported when specifying
&lt;code&gt;unit="s"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Support for non-nanosecond resolutions of timestamps is still actively developed. Many
methods relied on the assumption that a timestamp was always given in nanosecond resolution. It is
a lot of work to get rid of these problems everywhere and hence you might still encounter some
bugs in different areas.&lt;/p&gt;
&lt;h3 id="copy-on-write-improvements"&gt;Copy-on-Write improvements&lt;/h3&gt;
&lt;p&gt;Copy-on-Write (CoW) was originally introduced in pandas 1.5.0. Check out my initial post introducing
&lt;a href="https://phofl.github.io/cow-introduction.html"&gt;Copy-on-Write&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Short summary:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Any DataFrame or Series derived from another in&lt;/strong&gt; 
&lt;strong&gt;any way always behaves as a copy&lt;/strong&gt;. As a consequence, we can only change the values of an object 
through modifying the object itself. CoW disallows updating a DataFrame or a Series that shares 
data with another DataFrame or Series object inplace.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Version 1.5 provided the general mechanism but not much apart from that. A couple of bugs where 
Copy-on-Write was not respected, and hence two objects could get modified with one operation, were
discovered and fixed since then.&lt;/p&gt;
&lt;p&gt;More importantly, nearly all methods now utilize a &lt;em&gt;lazy copy&lt;/em&gt; mechanism to avoid copying the
underlying data as long as possible. Without CoW enabled, most methods perform defensive copies 
to avoid side effects when an object is modified later on. This results in high memory usage and a
relatively high runtime. Copy-on-Write enables us to remove all defensive copies and defer
the actual copies until the data of an object are modified.&lt;/p&gt;
&lt;p&gt;Additionally, CoW provides a cleaner and easier to work with API and should give your code a
performance boost on top of it. Generally, if an application does not rely on updating more than one object at
once and does not utilize chained assignment, the risk of turning Copy-on-Write 
on is minor. I've tested it on some code-bases and saw promising performance improvements, so I'd recommend
trying it out to see how it impacts your code. We are currently planning on making CoW the default in the next
major release. I'd recommend developing new features with Copy-on-Write enabled
to avoid migration issues later on.&lt;/p&gt;
&lt;p&gt;A PDEP (pandas development enhancement proposal) was submitted to deprecate and remove the
&lt;code&gt;inplace&lt;/code&gt; and &lt;code&gt;copy&lt;/code&gt; keyword in most methods. Those would become obsolete with Copy-on-Write
enabled and would only add confusion for users. You can follow this discussion 
&lt;a href="https://github.com/pandas-dev/pandas/pull/51466"&gt;here&lt;/a&gt;. If accepted, the removal of both keywords
will happen when CoW is made the default.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;pandas 2.0 brings many new and exiting features. We've seen a couple of them and looked at how
to utilize them. &lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out in the comments to share your thoughts and feedback 
on the 2.0 release. I will write additional posts focusing on Copy-on-Write and how to get the
most out of it. Follow me on Medium if you like to read more about pandas in general.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category></entry><entry><title>A guide to efficient data selection in pandas</title><link href="https://phofl.github.io/indexing-copy-view.html" rel="alternate"></link><published>2023-02-10T00:00:00+01:00</published><updated>2023-02-10T00:00:00+01:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-02-10:/indexing-copy-view.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Improve performance when selecting data from a pandas object&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There exist different ways of selecting a subset of data from a pandas object. Depending
on the specific operation, the result will either be a view pointing to the original
data or a copy of the original data. This ties …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Improve performance when selecting data from a pandas object&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There exist different ways of selecting a subset of data from a pandas object. Depending
on the specific operation, the result will either be a view pointing to the original
data or a copy of the original data. This ties directly to the efficiency of the operation. 
The copy and view rules are partially derived from the 
&lt;a href="https://numpy.org/doc/stable/user/basics.indexing.html"&gt;NumPy advanced indexing rules&lt;/a&gt;.
We will look at different operations and how to improve performance and efficiency as much as 
possible. &lt;a href="https://github.com/phofl"&gt;I am&lt;/a&gt; a member of the pandas core team.&lt;/p&gt;
&lt;p&gt;We will also investigate how 
&lt;a href="https://medium.com/towards-data-science/a-solution-for-inconsistencies-in-indexing-operations-in-pandas-b76e10719744"&gt;Copy on Write&lt;/a&gt; 
will change the behavior for some operations to improve performance and avoid copies as much as 
possible.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We will use a dataset that contains all players from FIFA 2021. You can download the
dataset &lt;a href="https://www.kaggle.com/datasets/stefanoleone992/fifa-21-complete-player-dataset"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

df = pd.read_csv(&amp;quot;players_21.csv&amp;quot;, index_col=&amp;quot;team_position&amp;quot;).sort_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set each player's position as index and sort the &lt;code&gt;DataFrame&lt;/code&gt; by it.
This will allow faster and easier access to the players by position and will help us to
illustrate a few examples.&lt;/p&gt;
&lt;h2 id="selecting-a-subset-of-rows"&gt;Selecting a subset of rows&lt;/h2&gt;
&lt;p&gt;We start by selecting players by position from our dataset. There are a couple of ways to achieve
this. The most common might be selecting by a boolean mask. We can calculate the boolean mask 
to select all players with position &lt;code&gt;"LS"&lt;/code&gt; through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;mask = df.index == &amp;quot;LS&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Afterwards, we can extract the rows from our DataFrame by:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result1 = df[mask]
result2 = df.loc[mask]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both operations achieve the same result in this case. We will investigate the differences
when looking at modifying our DataFrame.&lt;/p&gt;
&lt;p&gt;Selecting rows by a boolean mask &lt;strong&gt;always&lt;/strong&gt; creates a copy of the data. Depending on the
size of your dataset, this might cause a significant slowdown. Alternatively, we can select the
data by slicing the object:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = df.loc[&amp;quot;LS&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Slicing the object creates a view on the underlying data, which thus makes your operation
significantly faster. You can also select every second/n-th row by:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = df.iloc[slice(1, len(df), 2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will also create a view pointing to the original object. Getting a view is generally preferable, because
it improves performance and reduces memory usage. On the other hand side, you could also
create a list of integers corresponding with our slice:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = df.iloc[list(range(1, len(df), 2))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Selecting rows by a list of integers will create a copy, even though the operation look similar and
returns exactly the same data. This is again derived from NumPy's indexing rules.&lt;/p&gt;
&lt;p&gt;Slicing has many applications, for example
by integer position, with a DatetimeIndex or slicing an Index with strings. Selecting data by 
slice, if possible, is significantly faster than with a list of integers or boolean masks.&lt;/p&gt;
&lt;p&gt;Summarizing, depending on your use case, you may be able to significantly improve performance
when selecting rows. Setting an appropriate index might make your operations easier to
read and more efficient. &lt;/p&gt;
&lt;h2 id="selecting-a-subset-of-columns"&gt;Selecting a subset of columns&lt;/h2&gt;
&lt;p&gt;There are generally two cases to consider when selecting columns from your DataFrame:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;selecting a single column&lt;/li&gt;
&lt;li&gt;selecting multiple columns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Selecting a single column is relatively straightforward, you can either use a regular &lt;strong&gt;getitem&lt;/strong&gt;
or &lt;code&gt;loc&lt;/code&gt; for this. There is no substantial difference for a single column when selecting data, 
only when we want to update said data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = df[&amp;quot;long_name&amp;quot;]
result = df.loc[:, &amp;quot;long_name&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As soon as an iterable is passed to one of both calls, or if the selected column is duplicated,
we get a DataFrame back, but a copy of the underlying data is made, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = df.loc[:, [&amp;quot;short_name&amp;quot;, &amp;quot;long_name&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Selecting more than one column generally makes a copy right now. All these operations will return
views when &lt;code&gt;Copy-on-Write&lt;/code&gt; is enabled. This will improve performance significantly for lager 
objects.&lt;/p&gt;
&lt;h2 id="assigning-data-to-a-subset-of-the-dataframe"&gt;Assigning data to a subset of the DataFrame&lt;/h2&gt;
&lt;p&gt;Let's look at how to update a subset of your DataFame efficiently. There are two general 
possibilities: A regular &lt;strong&gt;setitem&lt;/strong&gt; or using &lt;code&gt;loc&lt;/code&gt; / &lt;code&gt;iloc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When adding a new column to a DataFrame, I would suggest using a regular &lt;strong&gt;setitem&lt;/strong&gt; operation.
It is shorter and a bit easier to read. There is no substantial difference in both operations, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;new_column&amp;quot;] = 100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a substantial difference when updating a DataFrame though. Assume we want to set the
name for all players with position &lt;code&gt;"LS"&lt;/code&gt; in our object. A regular &lt;strong&gt;setitem&lt;/strong&gt; operation 
&lt;strong&gt;never&lt;/strong&gt; writes into the underlying array. The data of this column are copied before the update 
happens. Also, there is no way of updating a subset of a specific row in one operation. You'd have
to use chained assignment, which has its own pitfalls. We will investigate them later. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;long_name = df[[&amp;quot;long_name&amp;quot;]]
long_name[long_name.index == &amp;quot;LS&amp;quot;] = &amp;quot;Testname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are copying the whole column before updating all rows that have index &lt;code&gt;"LS"&lt;/code&gt; inplace. This
is significantly slower that using &lt;code&gt;loc&lt;/code&gt; / &lt;code&gt;iloc&lt;/code&gt;. Both methods update the underlying array
inplace if possible. Additionally, we don't have to use a boolean mask to achieve this.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.loc[&amp;quot;LS&amp;quot;, &amp;quot;long_name&amp;quot;] = &amp;quot;Testname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, &lt;code&gt;iloc&lt;/code&gt; is more efficient than &lt;code&gt;loc&lt;/code&gt;. The downside is, that you already have to
know the positions where you want to insert your new values. But if you want to update a specific
set of rows, using &lt;code&gt;iloc&lt;/code&gt; is more efficient than &lt;code&gt;loc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Setting values inplace without making a copy only works, if the dtype of the value/values to
set is compatible with the dtype of the underlying array. For example, setting integer values into
a float or object dtype column generally operates inplace. Setting a float value into an integer dtype
column has to copy the data as well. An integer column can't hold a float value, and hence the
data have to be cast into a dtype that can hold both values. As a side-note: There is an ongoing 
&lt;a href="https://github.com/pandas-dev/pandas/pull/50424"&gt;discussion&lt;/a&gt;
about deprecating this behavior and raise an error, if an incompatible value is set into a column. It
would require casting the column explicitly to float before setting the values. Feedback on this
proposal is welcome!&lt;/p&gt;
&lt;p&gt;There is one specific exception: When overwriting a whole column, using a regular &lt;strong&gt;setitem&lt;/strong&gt;
is generally faster than using &lt;code&gt;loc&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;long_name&amp;quot;] = &amp;quot;Testname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason for this is pretty simple: &lt;code&gt;loc&lt;/code&gt; writes into the underlying array, which means that
you have to update every row for this column. The above operation simply swaps out the old column 
and adds the new column to the object without copying anything.&lt;/p&gt;
&lt;h2 id="chained-assignment"&gt;Chained assignment&lt;/h2&gt;
&lt;p&gt;Chained assignment describes doing two indexing operations with one statement and then assigning
data to the selected subset, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;long_name&amp;quot;][df.index == &amp;quot;LS&amp;quot;] = &amp;quot;Testname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation updates the DataFrame accordingly. In general, chained assignment shouldn't be
used, because it is the frequent culprit behind the &lt;code&gt;SettingWithCopyWarning&lt;/code&gt;. Additionally,
chained assignment will raise an error with copy on write enabled globally or as soon as 
copy on write becomes the default.&lt;/p&gt;
&lt;h2 id="performance-comparison"&gt;Performance comparison&lt;/h2&gt;
&lt;p&gt;Let's look at what this means performance-wise. This is just meant as a quick example to
show how to improve the efficiency of your data selections through avoiding copies. You'll have to 
tailor this to your application. &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;iloc&lt;/code&gt; are really flexible, so use-cases
will vary a lot. &lt;/p&gt;
&lt;p&gt;We need larger DataFrames
to avoid noise in our operations. We instantiate a DataFrame with random numbers:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np

df = pd.DataFrame(
    np.random.randint(1, 100, (1_000_000, 30)), 
    columns=[f&amp;quot;col_{i}&amp;quot; for i in range(30)],
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's look what slicing vs. selecting a list of integers means performance-wise:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;%timeit df.loc[slice(10_000, 900_000)]
9.61 µs ± 493 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
%timeit df.loc[list(range(10_000, 900_000))]
68.2 ms ± 465 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a pretty significant difference for a small change to your code.
Using &lt;code&gt;iloc&lt;/code&gt; shows the same difference.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;You can speed up your data selection and data modification methods through choosing the
best method for your operation. Generally, using a slice to select rows from a DataFrame is
significantly faster than using a boolean mask or a list of integers. When setting values, you
have to be careful to use compatible values. Additionally, we can improve performance by using
&lt;code&gt;loc&lt;/code&gt; or &lt;code&gt;iloc&lt;/code&gt;, if we don't have a problem with modifying the underlying array.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category></entry><entry><title>A solution for inconsistencies in indexing operations in pandas</title><link href="https://phofl.github.io/cow-introduction.html" rel="alternate"></link><published>2022-12-23T00:00:00+01:00</published><updated>2022-12-23T00:00:00+01:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2022-12-23:/cow-introduction.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Get rid of annoying SettingWithCopyWarning messages&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Indexing operations in pandas are quite flexible and thus, have many cases that can behave quite 
different and therefore produce unexpected results. Additionally, it is hard to predict when a 
&lt;code&gt;SettingWithCopyWarningis&lt;/code&gt; raised and what this means exactly. I’ll show a couple of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Get rid of annoying SettingWithCopyWarning messages&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Indexing operations in pandas are quite flexible and thus, have many cases that can behave quite 
different and therefore produce unexpected results. Additionally, it is hard to predict when a 
&lt;code&gt;SettingWithCopyWarningis&lt;/code&gt; raised and what this means exactly. I’ll show a couple of different 
scenarios and how each operation might impact your code. Afterwards, we will look at a new feature 
called &lt;code&gt;Copy on Write&lt;/code&gt; that helps you to get rid of the inconsistencies and of 
&lt;code&gt;SettingWithCopyWarnings&lt;/code&gt;. We will also investigate how this might impact performance and other 
methods in general.&lt;/p&gt;
&lt;h2 id="indexing-operations"&gt;Indexing operations&lt;/h2&gt;
&lt;p&gt;Let’s look at how indexing operations currently work in pandas. If you are already familiar with 
indexing operations, you can jump to the next section. But be aware, there are many cases with 
different forms of behavior. The exact behavor is hard to predict.&lt;/p&gt;
&lt;p&gt;An operation in pandas produces a copy, when the underlying data of the parent DataFrame and the 
new DataFrame are not shared. A view is an object that does share data with the parent object. A 
modification to the view can potentially impact the parent object.&lt;/p&gt;
&lt;p&gt;As of right now, some indexing operations return copies while others return views. The exact 
behavior is hard to predict, even for experienced users. This has been a big annoyance for me in 
the past.&lt;/p&gt;
&lt;p&gt;Let’s start with a DataFrame with two columns:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame({&amp;quot;user_id&amp;quot;: [1, 2, 3], &amp;quot;score&amp;quot;: [10, 15, 20]})

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;strong&gt;getitem&lt;/strong&gt; operation on a DataFrame or Series returns a subset of the initial object. The subset 
might consist of one or a set of columns, one or a set of rows or a mixture of both. A &lt;strong&gt;setitem&lt;/strong&gt; 
operation on a DataFrame or Series updates a subset of the initial object. The subset itself is 
defined by the arguments to the calls.&lt;/p&gt;
&lt;p&gt;A regular &lt;strong&gt;getitem&lt;/strong&gt; operation on a DataFrame provides a view in most cases:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;view = df[&amp;quot;user_id&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a consequence, the new object &lt;code&gt;view&lt;/code&gt; still references the parent object &lt;code&gt;df&lt;/code&gt; and its data. Hence, 
writing into the view will also modify the parent object.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;view.iloc[0] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;strong&gt;setitem&lt;/strong&gt; operation will consequently update not only our &lt;code&gt;view&lt;/code&gt; but also &lt;code&gt;df&lt;/code&gt;. This 
happens because the underlying data are shared between both objects.&lt;/p&gt;
&lt;p&gt;This is only true, if the column &lt;code&gt;user_id&lt;/code&gt; occurs only once in &lt;code&gt;df&lt;/code&gt;. As soon as &lt;code&gt;user_id&lt;/code&gt; is 
duplicated the &lt;strong&gt;getitem&lt;/strong&gt; operation returns a DataFrame. This means the returned object is a copy 
instead of a view:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame(
    [[1, 10, 2], [3, 15, 4]], 
    columns=[&amp;quot;user_id&amp;quot;, &amp;quot;score&amp;quot;, &amp;quot;user_id&amp;quot;],
)
not_a_view = df[&amp;quot;user_id&amp;quot;]
not_a_view.iloc[0] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;setitem&lt;/strong&gt; operation does not update &lt;code&gt;df&lt;/code&gt;. We also get our first &lt;code&gt;SettingWithCopyWarning&lt;/code&gt;, even 
though this is a perfectly acceptable operation. The &lt;strong&gt;getitem&lt;/strong&gt; operation itself has many more cases, 
like list-like keys, e.g. &lt;code&gt;df[["user_id"]]&lt;/code&gt;, MultiIndex-columns and many more. I will go into more 
detail in follow-up posts to look at different forms of performing indexing operations and their 
behavior.&lt;/p&gt;
&lt;p&gt;Let’s have a look at another case that is a bit more complicated than a single &lt;strong&gt;getitem&lt;/strong&gt; operation: 
chained indexing. Chained indexing means filtering with a boolean mask followed by a &lt;strong&gt;getitem&lt;/strong&gt; 
operation or the other way around. This is done in one step. We do not create a new variable to 
store the result of the first operation.&lt;/p&gt;
&lt;p&gt;We again start with a regular DataFrame:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame({&amp;quot;user_id&amp;quot;: [1, 2, 3], &amp;quot;score&amp;quot;: [10, 15, 20]})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can update all &lt;code&gt;user_ids&lt;/code&gt; that have a score greater than 15 through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;user_id&amp;quot;][df[&amp;quot;score&amp;quot;] &amp;gt; 15] = 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take the column &lt;code&gt;user_id&lt;/code&gt; and apply the filter afterwards. This works perfectly fine, because 
the column selection creates a view and the &lt;strong&gt;setitem&lt;/strong&gt; operation updates said view. We can switch 
both operations as well:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;score&amp;quot;] &amp;gt; 15][&amp;quot;user_id&amp;quot;] = 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This execution order produces another &lt;code&gt;SettingWithCopyWarning&lt;/code&gt;. In contrast to our earlier example, 
nothing happens. The DataFrame &lt;code&gt;df&lt;/code&gt; is not modified. This is a silent no-operation. The boolean 
mask always creates a copy of the initial DataFrame. Hence, the initial &lt;strong&gt;getitem&lt;/strong&gt; operation returns 
a copy. The return value is not assigned to any variable and is only a temporary result. The 
setitem operation updates this temporary copy. As a result, the modification is lost. The fact 
that masks return copies while column selections return views is an implementation detail. 
Ideally, such implementation details should not be visible.&lt;/p&gt;
&lt;p&gt;Another approach of doing this is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;new_df = df[df[&amp;quot;score&amp;quot;] &amp;gt; 15]
new_df[&amp;quot;user_id&amp;quot;] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation updates &lt;code&gt;new_df&lt;/code&gt; as intended but shows a &lt;code&gt;SettingWithCopyWarning&lt;/code&gt; anyway, because we 
can not update &lt;code&gt;df&lt;/code&gt;. Most of us probably never want to update the initial object (e.g. &lt;code&gt;df&lt;/code&gt;) in this 
scenario, but we get the warning anyway. In my experience this leads to unnecessary copy statements 
scattered over the code base.&lt;/p&gt;
&lt;p&gt;This is just a small sample of current inconsistencies and annoyances in indexing operations.&lt;/p&gt;
&lt;p&gt;Since the actual behavior is hard to predict, this forces many defensive copies in other methods. 
For example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dropping of columns&lt;/li&gt;
&lt;li&gt;setting a new index&lt;/li&gt;
&lt;li&gt;resetting the index&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All copy the underlying data. These copies are not necessary from an implementation perspective. 
The methods could return views pretty easily, but returning views would lead to unpredictable 
behavior later on. Theoretically, one &lt;strong&gt;setitem&lt;/strong&gt; operation could propagate through the whole 
call-chain, updating many DataFrames at once.&lt;/p&gt;
&lt;h2 id="copy-on-write"&gt;Copy on Write&lt;/h2&gt;
&lt;p&gt;Let’s look at how a new feature called “Copy on Write” (CoW) helps us to get rid of these 
inconsistencies in our code base. CoW means that &lt;strong&gt;any DataFrame or Series derived from another in&lt;/strong&gt; 
&lt;strong&gt;any way always behaves as a copy&lt;/strong&gt;. As a consequence, we can only change the values of an object 
through modifying the object itself. CoW disallows updating a DataFrame or a Series that shares 
data with another DataFrame or Series object inplace. With this information, we can again look at 
our initial example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame({&amp;quot;user_id&amp;quot;: [1, 2, 3], &amp;quot;score&amp;quot;: [10, 15, 20]})
view = df[&amp;quot;user_id&amp;quot;]
view.iloc[0] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;getitem&lt;/strong&gt; operation provides a view onto &lt;code&gt;df&lt;/code&gt; and it’s data. The &lt;strong&gt;setitem&lt;/strong&gt; operation triggers a copy 
of the underlying data before &lt;code&gt;10&lt;/code&gt; is written into the first row. Hence, the operation won't modify 
&lt;code&gt;df&lt;/code&gt;. An advantage of this behavior is, that we don’t have to worry about &lt;code&gt;user_id&lt;/code&gt; being potentially
duplicated or using &lt;code&gt;df[["user_id"]]&lt;/code&gt; instead of &lt;code&gt;df["user_id"]&lt;/code&gt;. All these cases behave exactly the 
same and no annoying warning is shown.&lt;/p&gt;
&lt;p&gt;Triggering a copy before updating the values of the object has performance implications. This 
will most certainly cause a small slowdown for some operations. On the other side, a lot of other 
operations can &lt;strong&gt;avoid&lt;/strong&gt; defensive copies and thus improve performance tremendously. The following 
operations can all return views with CoW:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dropping columns&lt;/li&gt;
&lt;li&gt;setting a new index&lt;/li&gt;
&lt;li&gt;resetting the index&lt;/li&gt;
&lt;li&gt;and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s consider the following DataFrame:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;na = np.array(np.random.rand(1_000_000, 100))
cols = [f&amp;quot;col_{i}&amp;quot; for i in range(100)]
df = pd.DataFrame(na, columns=cols)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;add_prefix&lt;/code&gt; adds the given string (e.g. &lt;code&gt;test&lt;/code&gt;) to the beginning of every column name:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.add_prefix(&amp;quot;test&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without CoW, this will copy the data internally. This is not necessary when looking solely at the 
operation. But since returning a view can have side effects, the method returns a copy. As a 
consequence, the operation itself is pretty slow:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;482 ms ± 3.43 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes quite long. We practically only modify 100 string literals without touching the data at 
all. Returning a view provides a significant speedup in this scenario:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;46.4 µs ± 1.04 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same operation runs multiple orders of magnitude faster. More importantly, the running time of 
&lt;code&gt;add_prefix&lt;/code&gt; is &lt;strong&gt;constant&lt;/strong&gt; when using CoW and does not depend on the size of your DataFrame. This 
operation was run on the main branch of pandas.&lt;/p&gt;
&lt;p&gt;The copy is only necessary, if two different objects share the same underlying data. In the 
example above, &lt;code&gt;view&lt;/code&gt; and &lt;code&gt;df&lt;/code&gt; both reference the same data. If the data is exclusive to one &lt;code&gt;DataFrame&lt;/code&gt; 
object, no copy is needed, we can continue to modify the data inplace:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame({&amp;quot;user_id&amp;quot;: [1, 2, 3], &amp;quot;score&amp;quot;: [10, 15, 20]})
df.iloc[0] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the &lt;strong&gt;setitem&lt;/strong&gt; operation will continue to operate inplace without triggering a copy.&lt;/p&gt;
&lt;p&gt;As a consequence, all the different scenarios that we have seen initially have exactly the same 
behavior now. We don’t have to worry about subtle inconsistencies anymore.&lt;/p&gt;
&lt;p&gt;Another case that currently has strange and hard to predict behavior is chained indexing. Chained 
indexing under CoW will &lt;strong&gt;never&lt;/strong&gt; work. This is a direct consequence of the CoW mechanism. The initial 
selection of columns might return a view, but a copy is triggered when we perform the subsequent 
setitem operation. Fortunately, we can easily modify our code to avoid chained indexing:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[&amp;quot;user_id&amp;quot;][df[&amp;quot;score&amp;quot;] &amp;gt; 15] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use &lt;code&gt;loc&lt;/code&gt; to do both operations at once:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.loc[df[&amp;quot;score&amp;quot;] &amp;gt; 15, &amp;quot;user_id&amp;quot;] = 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summarizing, every object that we create behaves like a copy of the parent object. We can not 
accidentally update an object other than the one we are currently working with.&lt;/p&gt;
&lt;h2 id="how-to-try-it-out"&gt;How to try it out&lt;/h2&gt;
&lt;p&gt;You can try the CoW feature since pandas 1.5.0. Development is still ongoing, but the general 
mechanism works already.&lt;/p&gt;
&lt;p&gt;You can either set the CoW flag globally through on of the following statements:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.set_option(&amp;quot;mode.copy_on_write&amp;quot;, True)
pd.options.mode.copy_on_write = True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you can enable CoW locally with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with pd.option_context(&amp;quot;mode.copy_on_write&amp;quot;, True):
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen that indexing operations in pandas have many edge cases and subtle differences in 
behavior that are hard to predict. CoW is a new feature aimed at addressing those differences. 
It can potentially impact performance positively or negatively based on what we are trying to do 
with our data. The full proposal for CoW can be found 
&lt;a href="https://docs.google.com/document/d/1ZCQ9mx3LBMy-nhwRl33_jgcvWo9IWdEfxDNQ2thyTb0/edit#heading=h.iexejdstiz8u"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out to share your thoughts and feedback 
on indexing and Copy on Write. I will write follow-up posts focused on this topic and pandas in 
general.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category></entry></feed>