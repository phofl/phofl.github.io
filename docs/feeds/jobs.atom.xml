<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Hoefler - jobs</title><link href="https://phofl.github.io/" rel="alternate"></link><link href="https://phofl.github.io/feeds/jobs.atom.xml" rel="self"></link><id>https://phofl.github.io/</id><updated>2023-09-01T00:00:00+02:00</updated><entry><title>Reduce training time for CPU intensive models with scikit-learn and Coiled Functions</title><link href="https://phofl.github.io/coiled-functions-sklearn.html" rel="alternate"></link><published>2023-09-01T00:00:00+02:00</published><updated>2023-09-01T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-09-01:/coiled-functions-sklearn.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-functions-scikit-learn-snippet.png"&gt;&lt;/p&gt;
&lt;p&gt;You can use &lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Run&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/functions.html"&gt;Coiled Functions&lt;/a&gt;
for easily running scripts and functions on a VM in the cloud.&lt;/p&gt;
&lt;p&gt;In this post we'll use Coiled Functions to train a RandomForestClassifer on a cloud-hosted
machine that has enough cores to speed up our training process. The model parallelizes very well …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-functions-scikit-learn-snippet.png"&gt;&lt;/p&gt;
&lt;p&gt;You can use &lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Run&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/functions.html"&gt;Coiled Functions&lt;/a&gt;
for easily running scripts and functions on a VM in the cloud.&lt;/p&gt;
&lt;p&gt;In this post we'll use Coiled Functions to train a RandomForestClassifer on a cloud-hosted
machine that has enough cores to speed up our training process. The model parallelizes very well,
which means that training time on my local machine is only bound by the number of cores available.
Offloading this step onto a machine with 128 cores will save a lot of time during the training
process.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We will use random generated data for the purpose of this blog post. We will first train the model
locally, before we use Coiled Funtions to offload the calculation to AWS.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

def train():
    X, y = make_classification(n_samples=2_000_000, n_features=30, random_state=0, shuffle=False)
    clf = RandomForestClassifier(random_state=0, n_jobs=-1)
    clf.fit(X, y)
    return clf

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use 2 million samples and 30 features. This model takes around 12 minutes on my MacBook Air with
8 cores. The runtime can grow increasingly large if our dataset increases in size. We can speed up
that process through using a more powerful VM.&lt;/p&gt;
&lt;h2 id="using-coiled-functions-to-train-the-model-on-a-large-vm"&gt;Using &lt;code&gt;coiled functions&lt;/code&gt; to train the model on a large VM&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Functions&lt;/a&gt; come into the equation since we need access to a machine with a lot of cores. 
Coiled can connect to AWS or GCP and thus, use all resources that are available there.
Let's train the same model as above, but this time on an EC2 instance with 128 cores.&lt;/p&gt;
&lt;p&gt;Adding a &lt;code&gt;@coiled.function&lt;/code&gt; decorator to the function that executes our training step is the only
modification we have to do. The decorator will tell Coiled that it should spin up a large VM on 
AWS and train the model there, and then return the trained model to our local machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

@coiled.function(
    vm_type=&amp;quot;c6i.32xlarge&amp;quot;, # 128 cores, compute optimized
    keepalive=&amp;quot;5 minutes&amp;quot;,  # keep alive to train more models if necessary
)
def train():
    X, y = make_classification(n_samples=2_000_000, n_features=30, random_state=0, shuffle=False)
    clf = RandomForestClassifier(random_state=0, n_jobs=-1)
    clf.fit(X, y)
    return clf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's execute the training step and return the model back to our local machine:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = train()

RandomForestClassifier(n_jobs=-1, random_state=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's take a look at the CPU utilization during the training step:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-functions-scikit-learn.png"&gt;&lt;/p&gt;
&lt;p&gt;There is no need to adjust the other functions. Coiled will run our function on a VM in the cloud with
enough resources.&lt;/p&gt;
&lt;p&gt;Let's take a brief look at the arguments to &lt;code&gt;coiled.function()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;vm_type&lt;/code&gt;: This specifies the type of &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;EC2 instance&lt;/a&gt;.
  We are looking for an instance that as many cores as possible to speed up our training step.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keepalive&lt;/code&gt;: Keeps the VM alive so that we can run multiple queries against the data in memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;coiled.function()&lt;/code&gt; will now start a VM in AWS with the specified EC2 instance. The VM is normally up
and running in 1-2 minutes. Coiled will scan our local environment and replicate the same 
dependencies on this machine. We don't have to specify an explicit Python environment. Inputs of 
your function are serialized and sent to the VM
as well. Coiled will return our results back to our local machine.&lt;/p&gt;
&lt;p&gt;Coiled would normally shut down the VM immediately after the Python interpreter finishes. This is mostly to
reduce costs. We specified
&lt;code&gt;keepalive="5 minutes"&lt;/code&gt; to keep the VM alive for a few minutes after our Python interpreter
finished. This ensures that new local runs can connect to the same VM avoiding
the boot time of up to 2 minutes; we call this a warm start.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;coiled functions&lt;/code&gt; enables you to run queries on a machine with as many cores as you need.
This grants you access to computational resources that can be hard to replicate locally.&lt;/p&gt;
&lt;p&gt;For more resources:
- Check out the &lt;a href="https://docs.coiled.io/user_guide/labs/functions.html"&gt;documentation&lt;/a&gt;
- Use Coiled functions to train a &lt;a href="https://medium.com/coiled-hq/how-to-train-a-neural-network-on-a-gpu-in-the-cloud-with-coiled-functions-40fa9aca723b"&gt;neural net on a powerful GPU&lt;/a&gt;
- Process &lt;a href="https://medium.com/coiled-hq/process-hundreds-of-gb-of-data-with-coiled-functions-and-duckdb-4b7df2f84d2f"&gt;process hundreds of GBs of data with DuckDB&lt;/a&gt; using Coiled functions&lt;/p&gt;</content><category term="posts"></category><category term="coiled"></category><category term="jobs"></category><category term="scikit-learn"></category></entry><entry><title>Process hundreds of GB of data with Coiled Functions and DuckDB</title><link href="https://phofl.github.io/coiled-functions-duckdb.html" rel="alternate"></link><published>2023-08-07T00:00:00+02:00</published><updated>2023-08-07T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-08-07:/coiled-functions-duckdb.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/duckdb-coiled-functions.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html"&gt;Coiled Jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Functions&lt;/a&gt;
for easily running scripts and functions in the cloud.&lt;/p&gt;
&lt;p&gt;In this post we'll use Coiled Functions to process the 150 GB &lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;Uber-Lyft dataset&lt;/a&gt; 
on a single machine with &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We start with creating the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/duckdb-coiled-functions.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html"&gt;Coiled Jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Functions&lt;/a&gt;
for easily running scripts and functions in the cloud.&lt;/p&gt;
&lt;p&gt;In this post we'll use Coiled Functions to process the 150 GB &lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;Uber-Lyft dataset&lt;/a&gt; 
on a single machine with &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We start with creating the SQL queries that we want to run against the data locally. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def load_data(conn):
  # Load data into memory so that subsequent queries are fast
  conn.execute(
    '''
    CREATE TABLE test AS 
    SELECT * FROM read_parquet(&amp;quot;s3://coiled-datasets/uber-lyft-tlc/*&amp;quot;)
    '''
  )


def compute_percentage_of_tipped_rides(conn):
    # Run the actual query
   return conn.execute(
       '''
       SELECT hvfhs_license_num, sum(tipped) / count(tipped)
        FROM (select 
           *,  
           CASE WHEN tips &amp;gt; 0.0 then 1 ELSE 0 end as tipped
           from test) GROUP BY hvfhs_license_num
       '''
   ).fetchall()


def create_conn():
    import duckdb

    return duckdb.connect()


def query_results():
    conn = create_conn()
    load_data(conn)
    return compute_percentage_of_tipped_rides(conn)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These queries aren't particularly fancy, they are meant to illustrate how we can process these
files. If we execute these queries as is, it would pull all the data onto our machine. The whole
dataset won't fit into memory on most workstations, so let's look at how Coiled can make this work. The loading
would take a long time, even if we had enough memory.&lt;/p&gt;
&lt;h2 id="using-coiled-functions-to-run-queries-on-a-large-vm"&gt;Using &lt;code&gt;coiled functions&lt;/code&gt; to run queries on a large VM&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.coiled.io/user_guide/labs/run.html"&gt;Coiled Functions&lt;/a&gt; come into the equation since we 
need access to machines that have enough resources and are also close to our data. 
Coiled can connect to AWS or GCP and thus, use all resources that are available there.
We will go through the necessary steps execute these queries on a VM in the same region as our data
with enough memory available.&lt;/p&gt;
&lt;p&gt;We'll have to adapt our &lt;code&gt;create_conn&lt;/code&gt; function to set 
&lt;a href="https://duckdb.org/docs/guides/import/s3_import.html"&gt;the environment variables needed by DuckDB&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def create_conn():
    import duckdb

    conn = duckdb.connect()
    conn.execute(&amp;quot;INSTALL httpfs&amp;quot;)
    conn.execute(&amp;quot;LOAD httpfs&amp;quot;)
    conn.execute(f&amp;quot;SET s3_region='us-east-2'&amp;quot;)
    conn.execute(f&amp;quot;SET s3_access_key_id='{os.environ['AWS_ACCESS_KEY_ID']}'&amp;quot;)
    conn.execute(f&amp;quot;SET s3_secret_access_key='{os.environ['AWS_SECRET_ACCESS_KEY']}'&amp;quot;)
    return conn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There will be &lt;a href="https://github.com/duckdblabs/duckdb_aws"&gt;an extension for DuckDB&lt;/a&gt; that can take 
care of this automatically starting from DuckDB 0.9.0.&lt;/p&gt;
&lt;p&gt;The next step is adding the &lt;code&gt;@coiled.function&lt;/code&gt; decorator to the function that executes our queries.
The decorator will tell Coiled that it should spin up a large VM on AWS and run the query there, and then return the result locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.function(
    vm_type=&amp;quot;m6i.16xlarge&amp;quot;, # 256 GB of RAM
    region=&amp;quot;us-east-2&amp;quot;,     # region of our data
    keepalive=&amp;quot;5 minutes&amp;quot;,  # keep alive to run multiple queries if necessary
)
def query_results():
    conn = create_conn()
    load_data(conn)
    return compute_percentage_of_tipped_rides(conn)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's execute our queries and pull the results back to our local machine:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = query_results()

print(result)
[
  ('HV0005', 0.1912300216459857), 
  ('HV0003', 0.1498555901186066), 
  ('HV0004', 0.09294857737045926), 
  ('HV0002', 0.08440046492889111),
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are now all in memory on our VM in the cloud:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/coiled_run_duckdb_memory.png"&gt;&lt;/p&gt;
&lt;p&gt;There is no need to adjust the other functions. Coiled will run our query on a VM in the cloud with
enough resources and close to our data.&lt;/p&gt;
&lt;p&gt;Let's take a brief look at the arguments to &lt;code&gt;coiled.function()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;vm_type&lt;/code&gt;: This specifies the type of &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;EC2 instance&lt;/a&gt;.
  We are looking for an instance that has enough memory to hold our data. This instance has 256GB,
  so this should be sufficient.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;region&lt;/code&gt;: The region specifies the AWS region that our VM is started in. Our data are also in
  &lt;code&gt;"us-east-2"&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keepalive&lt;/code&gt;: Keeps the VM alive so that we can run multiple queries against the data in memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;coiled.function()&lt;/code&gt; will now start a VM in AWS with the specified EC2 instance. The VM is normally up
and running in 1-2 minutes. Coiled will scan our local environment and replicate the same 
dependencies on this machine. We don't have to specify an explicit Python environment. Inputs of 
your function are serialized and sent to the VM
as well. Coiled will return our results back to our local machine.&lt;/p&gt;
&lt;p&gt;Coiled would normally shut down the VM immediately after the Python interpreter finishes. This is mostly to
reduce costs. We specified
&lt;code&gt;keepalive="5 minutes"&lt;/code&gt; to keep the VM alive for a few of minutes after our Python interpreter
finished. This ensures that new local runs can connect to the same VM avoiding
the boot time of up to 2 minutes; we call this a warm start.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;coiled functions&lt;/code&gt; enables you to run queries on a machine with as much memory as you want.
This grants you access to computational resources that can be very close to your data. Doing
data processing in the cloud becomes very easy with this functionality.&lt;/p&gt;
&lt;p&gt;You can check out the &lt;a href="https://docs.coiled.io/user_guide/labs/functions.html"&gt;docs&lt;/a&gt; 
or take a look at how to utilize Coiled Functions to train
a &lt;a href="https://medium.com/coiled-hq/how-to-train-a-neural-network-on-a-gpu-in-the-cloud-with-coiled-functions-40fa9aca723b"&gt;neural network on a powerful GPU&lt;/a&gt;.&lt;/p&gt;</content><category term="posts"></category><category term="coiled"></category><category term="jobs"></category><category term="duckdb"></category></entry><entry><title>How to Train a Neural Network on a GPU in the Cloud with coiled functions</title><link href="https://phofl.github.io/coiled-functions-pytorch.html" rel="alternate"></link><published>2023-07-24T00:00:00+02:00</published><updated>2023-07-24T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-07-24:/coiled-functions-pytorch.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going through the pains of setting up environments in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post will provide an example how to utilize &lt;code&gt;coiled functions&lt;/code&gt; to seamlessly train a 
&lt;strong&gt;neural network&lt;/strong&gt; on a GPU that is hosted in the cloud.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We have to start with creating a model on our local machine before we can start worrying about
training it. This blog post is not dedicated to figuring out a fancy mode, we will utilize the
&lt;code&gt;Net&lt;/code&gt; model that is given in the &lt;a href="https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html#training-your-pytorch-model"&gt;PyTorch tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can simply add the model definition to our python script. There is no need to do anything
different. Similarly, we will use the transformer that is given there as well.&lt;/p&gt;
&lt;p&gt;The next step is creating a function that we can use to train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train(transform):
    device = torch.device(&amp;quot;cpu&amp;quot;)
    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now train our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will train our model on the CPU of our local machine. The training is reasonably quick for such
a small model, but training time will grow exponentially as our model gets larger or if we are
using a significantly bigger dataset. Training the model on the CPU won't be sufficient anymore.
Additionally, there are a lot of machines out there that don't have GPUs built into them. For example, I'm using a MacBook Pro with an M2 CPU, which means my machine doesn't support &lt;code&gt;cuda&lt;/code&gt;.
Consequently, we need a different solution to make these steps accessible for folks who don't have
access to a local GPU.&lt;/p&gt;
&lt;h2 id="using-coiled-functions-to-train-the-model-on-a-cloud-hosted-gpu"&gt;Using &lt;code&gt;coiled functions&lt;/code&gt; to train the model on a cloud-hosted GPU&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;Coiled functions&lt;/a&gt; come into the equation if you 
need access to resources that aren't available
locally. Coiled can connect to AWS or GCP and thus, use all resources that are available there.
We will go through the necessary steps to train our model on a GPU that is hosted on AWS instead
of our local CPU. &lt;/p&gt;
&lt;p&gt;The first step includes defining a Python environment to run our computations. We simply include
PyTorch, CUDA, and Coiled, that's it. Generally, you should use the same Python version that is
installed locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

coiled.create_software_environment(
    name=&amp;quot;pytorch&amp;quot;,
    conda={
        &amp;quot;channels&amp;quot;: [&amp;quot;pytorch&amp;quot;, &amp;quot;nvidia&amp;quot;, &amp;quot;conda-forge&amp;quot;, &amp;quot;defaults&amp;quot;],
        &amp;quot;dependencies&amp;quot;: [
            &amp;quot;python=3.11&amp;quot;,
            &amp;quot;coiled&amp;quot;,
            &amp;quot;pytorch&amp;quot;,
            &amp;quot;torchvision&amp;quot;,
            &amp;quot;torchaudio&amp;quot;,
            &amp;quot;cudatoolkit&amp;quot;,
            &amp;quot;pynvml&amp;quot;,
        ],
    },
    gpu_enabled=True,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled will create a Python environment for you. This step is only necessary when running your 
script for the first time. The resulting environment is cached which makes further runs more
efficient.&lt;/p&gt;
&lt;p&gt;The next step is adding the &lt;code&gt;@coiled.run&lt;/code&gt; decorator to our training
function that tells our program we want to execute said function on a machine in the cloud.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;, # GPU instance type
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, we have to tell PyTorch that we want to train the model on the GPU.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train():
    import torch
    # tell PyTorch to use the GPU
    device = torch.device(&amp;quot;cuda:0&amp;quot;)
    ...
    return net.to(torch.device(&amp;quot;cpu&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting this all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;,
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
def train(transform):
    import torch
    device = torch.device(&amp;quot;cuda:0&amp;quot;)

    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net.to(torch.device(&amp;quot;cpu&amp;quot;))


if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's take a brief look at the arguments to &lt;code&gt;coiled.run()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;worker_vm_type&lt;/code&gt;: This specifies the type of &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;EC2 instance&lt;/a&gt;.
  We are looking for an instance that has a GPU attached to it. The G5 family has Nvidia GPUs
  attached to it. The smallest version is sufficient for our example, but you can choose instances
  with up to 8 GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;region&lt;/code&gt;: The region specifies the AWS region that our VM is started in. We observed that &lt;code&gt;"us-west-2"&lt;/code&gt;
  is a region where GPUs are easier to get. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;software&lt;/code&gt;: This specifies the coiled software environment that is installed. This corresponds
  to the environment that we previously created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;coiled.run()&lt;/code&gt; will now start a VM in AWS with the specified EC2 instance. The VM is normally up
and running in 1-2 minutes. The previously specified Python environment is installed automatically.
Coiled executes the function on said VM. Inputs of your function are serialized and sent to the VM
as well. It makes sense to download the training data on the VM to reduce time that is spent sending
data to AWS. The function returns our model back to our local machine so that we can use it locally
without depending on AWS.&lt;/p&gt;
&lt;p&gt;Coiled will shut down the VM immediately after the Python interpreter finishes. This is mostly to
reduce costs. You can specify a certain amount of time that the VM is kept alive through 
&lt;code&gt;keepalive="5 minutes"&lt;/code&gt;. This ensures that new local runs can connect to the same VM avoiding
the boot time of up to 2 minutes; we call this a warm start.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;coiled functions&lt;/code&gt; enables you to seamlessly port the training process for a neural network
from your local machine to AWS or GCP. This grants everyone access to multiple GPUs or huge
machines independent of the local machine that is actually used. Training a neural network on a
GPU becomes as easy as adding a decorator to the training function.&lt;/p&gt;</content><category term="posts"></category><category term="coiled"></category><category term="jobs"></category><category term="gpu"></category><category term="machine learning"></category></entry></feed>