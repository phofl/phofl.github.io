<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Hoefler - dask</title><link href="https://phofl.github.io/" rel="alternate"></link><link href="https://phofl.github.io/feeds/dask.atom.xml" rel="self"></link><id>https://phofl.github.io/</id><updated>2024-05-27T00:00:00+02:00</updated><entry><title>Dask DataFrame is Fast Now</title><link href="https://phofl.github.io/dask-dataframe-is-fast-now.html" rel="alternate"></link><published>2024-05-27T00:00:00+02:00</published><updated>2024-05-27T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2024-05-27:/dask-dataframe-is-fast-now.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/dask-improvement.png"&gt;&lt;/p&gt;
&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;p&gt;Dask DataFrame scales out pandas DataFrames to operate at the 100GB-100TB scale.&lt;/p&gt;
&lt;p&gt;Historically, &lt;a href="https://www.dask.org/"&gt;Dask&lt;/a&gt; was pretty slow compared to other tools in this space (like Spark). Due to a number of improvements focused on performance,
it's now pretty fast (about 20x faster than before). The new implementation moved Dask â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/dask-improvement.png"&gt;&lt;/p&gt;
&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;p&gt;Dask DataFrame scales out pandas DataFrames to operate at the 100GB-100TB scale.&lt;/p&gt;
&lt;p&gt;Historically, &lt;a href="https://www.dask.org/"&gt;Dask&lt;/a&gt; was pretty slow compared to other tools in this space (like Spark). Due to a number of improvements focused on performance,
it's now pretty fast (about 20x faster than before). The new implementation moved Dask from getting destroyed by
Spark on every benchmark to regularly outperforming Spark on TPC-H queries by a significant margin.&lt;/p&gt;
&lt;p&gt;Dask DataFrame workloads struggled with many things. Performance and memory usage were
commonly seen pain points, shuffling was unstable for bigger datasets, making scaling out
hard. Writing efficient code required understanding too much of the internals of Dask.&lt;/p&gt;
&lt;p&gt;The new implementation changed all of this. Things that didn't work were completely rewritten from scratch and existing
implementations were improved upon. This puts Dask DataFrames on a solid foundation that
allows faster iteration cycles in the future.&lt;/p&gt;
&lt;p&gt;We'll go through the three most prominent changes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apache Arrow support&lt;/li&gt;
&lt;li&gt;Faster joins&lt;/li&gt;
&lt;li&gt;Query optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We'll cover how these changes impact performance and make it easier to use Dask efficiently, even for users that are new to distributed computing. We'll also discuss plans for future improvements.&lt;/p&gt;
&lt;h2 id="1-apache-arrow-support-efficient-string-datatype"&gt;1. Apache Arrow Support: Efficient String Datatype&lt;/h2&gt;
&lt;p&gt;A Dask DataFrame consists of many pandas DataFrames. Historically, pandas used NumPy for numeric data, 
but Python objects for text data, which are inefficient and blow up memory usage. Operations on 
object data also hold the GIL, which doesn't matter much for pandas, but is a catastrophy for 
performance with a parallel system like Dask.&lt;/p&gt;
&lt;p&gt;The pandas 2.0 release introduced support for general-purpose Arrow datatypes, so &lt;a href="https://docs.coiled.io/blog/pyarrow-in-pandas-and-dask.html"&gt;Dask now uses PyArrow-backed
strings by default&lt;/a&gt;.  These are &lt;em&gt;much&lt;/em&gt; better.  PyArrow strings reduce memory usage by up to 80% and
unlock multi-threading for string operations. Workloads that previously
struggled with available memory now fit comfortably in much less space, and are
a lot faster because they no longer constantly spill excess data to disk.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/arrow-strings-memory-usage.png"&gt;&lt;/p&gt;
&lt;h2 id="2-faster-joins-with-a-new-shuffle-algorithm"&gt;2. Faster Joins with a New Shuffle Algorithm&lt;/h2&gt;
&lt;p&gt;Shuffling is an essential component of distributed systems to enable sorting, joins, and complex group by operations. It is an all-to-all, network-intensive operation that's often the most expensive component in a workflow. Dask has a new shuffling system, which greatly impacts overall performance, especially on complex, data-intensive workloads.&lt;/p&gt;
&lt;p&gt;A shuffle operation is intrinsically an all-to-all communication operation where every
input partition has to provide a tiny slice of data to every output partition. Dask was already
using it's own task-based algorithm that managed to reduce the &lt;code&gt;O(n * n)&lt;/code&gt;  task
complexity to &lt;code&gt;O(log(n) * n)&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; is the number of partitions. This was a drastic
reduction in the number of tasks, but the non-linear scaling ultimately did not allow Dask to process
arbitrarily large datasets.&lt;/p&gt;
&lt;p&gt;Dask introduced a new &lt;a href="https://docs.coiled.io/blog/shuffling-large-data-at-constant-memory.html"&gt;P2P (peer-to-peer) shuffle  method&lt;/a&gt; that reduced the task complexity to &lt;code&gt;O(n)&lt;/code&gt;
which scales linearly with the size of the dataset and the size of the cluster. It also
incorporates an efficient disk integration which allows easily shuffling datasets which are much
larger than memory.  The new system is extremely stable and "just works" across any scale of data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/shuffle-memory-comparison.png"&gt;&lt;/p&gt;
&lt;h2 id="3-optimizer"&gt;3. Optimizer&lt;/h2&gt;
&lt;p&gt;Dask itself is lazy, which means that it registers your whole query before doing any actual work.
This is a powerful concept that enables a lot of optimizations, but historically Dask wasn't taking advantage of this
knowledge in the past. Dask also did a bad job of hiding internal complexities and
left users on their own while navigating the difficulties of distributed computing and running
large scale queries. It made writing efficient code painful for non-experts.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.dask.org/en/stable/changelog.html#query-planning"&gt;The Dask release in March&lt;/a&gt; includes a complete re-implementation of the DataFrame API to support query optimization. This is a big deal.
The new engine centers around a query optimizer that rewrites your code to make it more efficient and
better tailored to Dask's strengths. Let's dive into some optimization strategies, how they make
Dask run faster and scale better.&lt;/p&gt;
&lt;p&gt;We will start with a couple general-purpose optimizations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Column projection&lt;/li&gt;
&lt;li&gt;Filter pushdown&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then dive into more specific techniques that are tailored to distributed systems generally
and Dask more specifically:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Automatic partition resizing&lt;/li&gt;
&lt;li&gt;Trivial merge and join operations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(column-projection)=&lt;/p&gt;
&lt;h3 id="31-column-projection"&gt;3.1 Column Projection&lt;/h3&gt;
&lt;p&gt;Most datasets have more columns than needed. Dropping them requires foresight ("What columns will I need for this query? ðŸ¤”") so most people don't think about this when loading data. This is bad for performance because carrying around lots of excess data slows everything down.
Column Projection drops columns as soon as they aren't needed anymore. It's a straightforward optimization, but highly beneficial.&lt;/p&gt;
&lt;p&gt;The legacy implementation always reads all columns from storage and only drops columns when explicitly specified by the user.
Simply operating on less data is a big win for performance and memory usage.&lt;/p&gt;
&lt;p&gt;The optimizer looks at the query and figures out which columns are needed for each operation.
It looks at the final step of the query and then works backwards step by
step to the data source, injecting drop operations to get rid of unnecessary columns.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/projection.png"&gt;&lt;/p&gt;
&lt;h3 id="32-filter-pushdown"&gt;3.2 Filter Pushdown&lt;/h3&gt;
&lt;p&gt;Filter pushdown is another general-purpose optimization with
the same goal as column projection: operate on less data. The legacy implementation did not reorder filter operations. The new implementation executes filter operations as early as
possible while maintaining the same results.&lt;/p&gt;
&lt;p&gt;The optimizer identifies every filter in the query and looks at the previous operation to see if we
can move the filter closer to the data source. It will repeat this until it finds an operation that
can't be switched with a filter. This is a bit harder than
column projections, because Dask has to make sure that the operations don't change the values of the
DataFrame. For example, switching a filter and a merge operation is fine (values don't change), but switching a filter
and a replace operation is invalid, because the values might change and rows that would previously have been filtered out now won't be, or vice versa.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/filter.png"&gt;&lt;/p&gt;
&lt;p&gt;Additionally, if the filter is strong enough then Dask can potentially drop complete files in the IO step.
This is a best-case scenario, where an earlier filter brings a huge performance improvement and even
requires reading less data from remote storage.&lt;/p&gt;
&lt;h3 id="33-automatically-resizing-partitions"&gt;3.3 Automatically Resizing Partitions&lt;/h3&gt;
&lt;p&gt;In addition to implementing the common optimization techniques described above, weâ€™ve also improved a
common pain point specific to distributed systems generally and Dask users specifically: optimal partition sizes.&lt;/p&gt;
&lt;p&gt;Dask DataFrames consist of many small pandas DataFrames called &lt;em&gt;partitions&lt;/em&gt;. Often, the number of
partitions is decided for you and Dask users are advised to manually "repartition" after reducing
or expanding their data (for example by dropping columns, filtering data, or expanding with joins) (see the &lt;a href="https://docs.dask.org/en/stable/dataframe-best-practices.html#repartition-to-reduce-overhead"&gt;Dask docs&lt;/a&gt;).
Without this extra step,
the (usually small) overhead from Dask can become a bottleneck if the pandas DataFrames
become too small, making Dask workflows painfully slow.&lt;/p&gt;
&lt;p&gt;Manually controlling the partition size is a difficult task that we, as Dask users, shouldnâ€™t have
to worry about. It is also slow because it requires network transfer of some partitions.
Dask DataFrame now automatically does two things to help when the partitions get
too small:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Keeps the size of each partition constant, based on the ratio of data you want to compute vs.
  the original file size. If, for example, you filter out 80% of the original dataset, Dask will
  automatically combine the resulting smaller partitions into fewer, larger partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Combines too-small partitions into larger partitions, based on an absolute minimum
  (default is 75 MB). If, for example, your original dataset is split into many tiny files,
  Dask will automatically combine them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/automatic_repartitioning_1.png"&gt;&lt;/p&gt;
&lt;p&gt;The optimizer will look at the number of columns and the size of the data within those. It
calculates a ratio that is used to combine multiple files into one partition.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/automatic_repartitioning_2.png"&gt;&lt;/p&gt;
&lt;p&gt;This step is currently limited to IO operations (like reading in a parquet dataset), but we plan
to extend it to other operations that allow cheaply combining partitions.&lt;/p&gt;
&lt;h3 id="34-trivial-merge-and-join-operations"&gt;3.4 Trivial Merge and Join Operations&lt;/h3&gt;
&lt;p&gt;Merge and join operations are typically cheap on a single machine with pandas but expensive in a
distributed setting.  Merging data in shared memory is cheap, while merging data across a network is quite slow,
due to the shuffle operations explained earlier.&lt;/p&gt;
&lt;p&gt;This is one of the most expensive operations in a distributed system. The legacy implementation triggered
a network transfer of both input DataFrames for every merge operation. This is sometimes necessary, but very
expensive.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/avoiding-shuffles.png"&gt;&lt;/p&gt;
&lt;p&gt;The optimizer will determine when shuffling is necessary versus when
a trivial join is sufficient because the data is already aligned properly. This can make individual merges
an order of magnitude faster. This also applies to other operations that normally require a shuffle
like &lt;code&gt;groupby().apply()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Dask merges used to be inefficient, which caused long runtimes. The optimizer fixes this for
the trivial case where these operations happen after each other, but the technique isn't very
advanced yet. There is still a lot of potential for improvement.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/avoiding-shuffles-advanced.png"&gt;&lt;/p&gt;
&lt;p&gt;The optimizer will look at the expression and inject shuffle nodes where necessary to avoid
unnecessary shuffles.&lt;/p&gt;
&lt;h2 id="how-do-the-improvements-stack-up-compared-to-the-legacy-implementation"&gt;How do the improvements stack up compared to the legacy implementation?&lt;/h2&gt;
&lt;p&gt;Dask is now 20x faster than before.  This improvement applies to the entire
DataFrame API (not just isolated components), with no known
performance regressions.  Dask now runs workloads that were impossible to
complete in an acceptable timeframe before.  This performance boost is due to many
improvements all layered on top of each other.  It's not about doing one thing
especially well, but about doing nothing especially poorly.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_dataframe_fast/dask-improvement.png"&gt;&lt;/p&gt;
&lt;p&gt;Performance, while the most enticing improvement, is not the only thing that got better. The
optimizer hides a lot of complexity from the user and makes the transition from pandas to Dask a
lot easier because it's now much more difficult to write poorly performing code.
The whole system is more robust.&lt;/p&gt;
&lt;p&gt;The new architecture of the API is a lot easier to work with as well. The legacy implementation leaked
a lot of internal complexities into high-level API implementations, making changes cumbersome. Improvements
are almost trivial to add now.&lt;/p&gt;
&lt;h2 id="whats-to-come"&gt;What's to come?&lt;/h2&gt;
&lt;p&gt;Dask DataFrame changed a lot over the last 18 months. The legacy API was often difficult to work with and
struggled with scaling out. The new implementation dropped things that didn't work and
improved existing implementations. The heavy lifting is finished now, which allows for
faster iteration cycles to improve upon the status quo. Incremental improvements are now
trivial to add.&lt;/p&gt;
&lt;p&gt;A few things that are on the immediate roadmap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Auto repartitioning:&lt;/strong&gt; this is partially implemented, but there is more potential to choose a more
  efficient partition size during optimization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster Joins:&lt;/strong&gt; there's still lots of fine-tuning to be done here.
  For example, there is a PR in flight with a 30-40% improvement.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Join Reordering:&lt;/strong&gt; Dask doesn't do this yet, but it's on the immediate roadmap&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="learn-more"&gt;Learn more&lt;/h2&gt;
&lt;p&gt;This article focuses on a number of improvements to Dask DataFrame and how much faster and more reliable it is as a result. If you're choosing between Dask and other popular DataFrame tools, you might also consider:&lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="dataframe"></category></entry><entry><title>High Level Query Optimization in Dask</title><link href="https://phofl.github.io/high-level-query-optimization-in-dask.html" rel="alternate"></link><published>2023-08-04T00:00:00+02:00</published><updated>2023-08-04T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-08-04:/high-level-query-optimization-in-dask.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/dask-expr/dask-expr-introduction-title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dask DataFrame doesn't currently optimize your code for you (like Spark or a SQL database would). 
This means that users waste a lot of computation. Let's look at a common example
which looks ok at first glance, but is actually pretty inefficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd

df = dd â€¦&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/dask-expr/dask-expr-introduction-title.png"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dask DataFrame doesn't currently optimize your code for you (like Spark or a SQL database would). 
This means that users waste a lot of computation. Let's look at a common example
which looks ok at first glance, but is actually pretty inefficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,  # unnecessarily reads all rows and columns
)
result = (
    df[df.hvfhs_license_num == &amp;quot;HV0003&amp;quot;]    # could push the filter into the read parquet call
    .sum(numeric_only=True)
    [&amp;quot;tips&amp;quot;]                                # should read only necessary columns
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make this run much faster with a few simple steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
    columns=[&amp;quot;tips&amp;quot;],
)
result = df.tips.sum()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently, Dask DataFrame wouldn't optimize this for you, but a new effort that is built around
logical query planning in Dask DataFrame will do this for you. This article introduces some of
those changes that are developed in &lt;a href="https://github.com/dask-contrib/dask-expr"&gt;dask-expr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can install and try &lt;code&gt;dask-expr&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pip install dask-expr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are using the &lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;NYC Taxi&lt;/a&gt; 
dataset in this post.&lt;/p&gt;
&lt;h2 id="dask-expressions"&gt;Dask Expressions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/dask-contrib/dask-expr"&gt;Dask expressions&lt;/a&gt; provides a logical query planning layer on 
top of Dask DataFrames. Let's look at our initial example and investigate how we can improve the efficiency
through a query optimization layer. As noted initially, there are a couple of things that aren't ideal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are reading all rows into memory instead of filtering while reading the parquet files.&lt;/li&gt;
&lt;li&gt;We are reading all columns into memory instead of only the columns that are necessary.&lt;/li&gt;
&lt;li&gt;We are applying the filter and the aggregation onto all columns instead of only &lt;code&gt;"tips"&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The query optimization layer from &lt;code&gt;dask-expr&lt;/code&gt; can help us with that. It will look at this expression
and determine that not all rows are needed. An intermediate layer will transpile the filter into
a valid filter-expression for &lt;code&gt;read_parquet&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
)
result = df.sum(numeric_only=True)[&amp;quot;tips&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This still reads every column into memory and will compute the sum of every numeric column. The 
next optimization step is to push the column selection into the &lt;code&gt;read_parquet&lt;/code&gt; call as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    columns=[&amp;quot;tips&amp;quot;],
    filters=[(&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;==&amp;quot;, &amp;quot;HV0003&amp;quot;)],
)
result = df.sum(numeric_only=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a basic example that you could rewrite by hand. Use cases that are closer to real
workflows might potentially have hundreds of columns, which makes rewriting them very strenuous
if you need a non-trivial subset of them.&lt;/p&gt;
&lt;p&gt;Let's take a look at how we can achieve this. &lt;code&gt;dask-expr&lt;/code&gt; records the expression as given by the
user in an expression tree:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result.pprint()

Projection: columns='tips'
  Sum: numeric_only=True
    Filter:
      ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/'
      EQ: right='HV0003'
        Projection: columns='hvfhs_license_num'
          ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tree represents the expression as is. We can observe that we would read the whole dataset into
memory before we apply the projections and filters. One observation of note: It seems like we
are reading the dataset twice, but Dask is able to fuse tasks that are doing the same to avoid
computing these things twice. Let's reorder the expression to make it more efficient:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result.simplify().pprint()

Sum: numeric_only=True
  ReadParquet: path='s3://coiled-datasets/uber-lyft-tlc/' 
               columns=['tips'] 
               filters=[('hvfhs_license_num', '==', 'HV0003')] 

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks quite a bit simpler. &lt;code&gt;dask-expr&lt;/code&gt; reordered the query and pushed the filter and the column
projection into the &lt;code&gt;read_parquet&lt;/code&gt; call. We were able to remove quite a few steps from our expression
tree and make the remaining expressions more efficient as well. This represents the steps that
we did manually in the beginning. &lt;code&gt;dask-expr&lt;/code&gt; performs these steps for arbitrary many columns without
increasing the burden on the developers.&lt;/p&gt;
&lt;p&gt;These are only the two most common and easy to illustrate optimization techniques from &lt;code&gt;dask-expr&lt;/code&gt;. 
Some other useful optimizations are already available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;len(...)&lt;/code&gt; will only use the Index to compute the length; additionally we can ignore many operations
  that won't change the shape of a DataFrame, like a &lt;code&gt;replace&lt;/code&gt; call.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;set_index&lt;/code&gt; and &lt;code&gt;sort_values&lt;/code&gt; won't eagerly trigger computations.&lt;/li&gt;
&lt;li&gt;Better informed selection of &lt;code&gt;merge&lt;/code&gt; algorithms.&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are still adding more optimization techniques to make Dask DataFrame queries more efficient.&lt;/p&gt;
&lt;h2 id="try-it-out"&gt;Try it out&lt;/h2&gt;
&lt;p&gt;The project is in a state where interested users should try it out. We published a couple of 
releases. The API covers a big chunk of the Dask DataFrame API, and we keep adding more. 
We have already observed very impressive performance improvements for workflows that would benefit
from query optimization. Memory usage is down for these workflows as well.&lt;/p&gt;
&lt;p&gt;We are very much looking for feedback and potential avenues to improve the library. Please give it
a shot and share your experience with us.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dask-expr&lt;/code&gt; is not integrated into the main Dask DataFrame implementation yet. You can install it
with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pip install dask-expr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The API is very similar to what Dask DataFrame provides. It exposes mostly the same methods as
Dask DataFrame does. You can use the same methods in most cases.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import dask_expr as dd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find a list of supported operations in the 
&lt;a href="https://github.com/dask-contrib/dask-expr#api-coverage"&gt;Readme&lt;/a&gt;. This project is still very much
in progress. The API might change without warning. We are aiming for weekly releases to push new
features out as fast as possible.&lt;/p&gt;
&lt;h2 id="why-are-we-adding-this-now"&gt;Why are we adding this now?&lt;/h2&gt;
&lt;p&gt;Historically, Dask focused on flexibility and smart scheduling instead of query optimization. 
The distributed scheduler built into Dask uses sophisticated algorithms to ensure ideal scheduling
of individual tasks. It tries to ensure that your resources are utilized as efficient as possible.
The graph construction process enables Dask users to build very
flexible and complicated graphs that reach beyond SQL operations. The flexibility that is provided
by the &lt;a href="https://docs.dask.org/en/latest/futures.html"&gt;Dask futures API&lt;/a&gt; requires very intelligent
algorithms, but it enables users to build highly sophisticated graphs. The following picture shows
the graph for a credit risk model:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask-expr/graph_credit_risk_model.png"&gt;&lt;/p&gt;
&lt;p&gt;The nature of the powerful scheduler and the physical optimizations enables us to build very
complicated programs that will then run efficiently. Unfortunately, the nature of these optimizations 
does not enable us to avoid scheduling work that is not necessary. This is where the current effort
to build high level query optimization into Dask comes in.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Dask comes with a very smart distributed scheduler but without much logical query planning. This
is something we are rectifying now through building a high level query optimizer into Dask 
DataFrame. We expect to improve performance and reduce memory usage for an average Dask workflow.&lt;/p&gt;
&lt;p&gt;This API is read for interested users to play around with. It covers a good chunk of the DataFrame
API. The library is under active development, we expect to add many more interesting things over
the coming weeks and months. &lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="query optimizer"></category><category term="performance"></category></entry><entry><title>Dask performance benchmarking put to the test: Fixing a pandas bottleneck</title><link href="https://phofl.github.io/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html" rel="alternate"></link><published>2023-06-28T00:00:00+02:00</published><updated>2023-06-28T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-28:/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With an impact of 40% increased runtime, it blocked the release planned for the next day!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/performance-regression.png"&gt;&lt;/p&gt;
&lt;p&gt;Luckily, the other attendees still focused on coffee and breakfast, so we commandeered an abandoned table next to our booth and got to work.&lt;/p&gt;
&lt;h2 id="performance-testing-at-coiled"&gt;Performance testing at Coiled&lt;/h2&gt;
&lt;p&gt;The performance problem &lt;a href="https://github.com/coiled/benchmarks/issues/840"&gt;had been flagged&lt;/a&gt; by the automated performance testing for Dask that we developed at &lt;a href="https://www.coiled.io/?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Coiled&lt;/a&gt;.
If you have not read Guido Imperiale's &lt;a href="https://blog.coiled.io/blog/performance-testing.html?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;blog post&lt;/a&gt; on our approach to performance testing, here is a summary:
With &lt;a href="https://github.com/coiled/benchmarks"&gt;&lt;code&gt;coiled/benchmarks&lt;/code&gt;&lt;/a&gt;, we created a benchmark suite that contains a variety of common workloads and operations with Dask, including standardized ones like the &lt;a href="https://github.com/h2oai/db-benchmark"&gt;&lt;code&gt;h2oai/db-benchmark&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It also contains tooling that allows us to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatically &lt;a href="https://blog.coiled.io/blog/performance-testing.html#nightly-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;detect performance regressions&lt;/a&gt; in Dask and raise them as issues.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.coiled.io/blog/performance-testing.html#a-b-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Run A/B tests&lt;/a&gt; to assess the performance impact of different versions of Dask, upstream packages, or cluster configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the former started this journey, the latter will also come in handy soon.  &lt;/p&gt;
&lt;h2 id="identifying-the-problem"&gt;Identifying the problem&lt;/h2&gt;
&lt;p&gt;Our automated regression testing had alerted us that &lt;a href="https://github.com/coiled/benchmarks/blob/895a13db09eb3172155e7b1260a5698f2284f5b7/tests/benchmarks/test_h2o.py#L140-L151"&gt;&lt;code&gt;test_h2o.py::test_q8&lt;/code&gt;&lt;/a&gt; had experienced &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1542560550"&gt;a significant increase&lt;/a&gt; in runtime across all data sizes and file formats. 
From the &lt;a href="https://benchmarks.coiled.io?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;historical report&lt;/a&gt; of our benchmarking suite, we could see that &lt;code&gt;dask/dask&lt;/code&gt; and &lt;code&gt;dask/distributed&lt;/code&gt; were unlikely to be the culprit: 
Nothing had changed on &lt;code&gt;dask/dask&lt;/code&gt; when the performance started to degrade, and there was only one unrelated change on &lt;code&gt;dask/distributed&lt;/code&gt;. 
That left us with the Coiled platform and upstream packages as possible candidates. &lt;/p&gt;
&lt;p&gt;After digging deeper into the cluster data, we noticed that &lt;code&gt;pandas&lt;/code&gt; had been upgraded from &lt;code&gt;1.5.3&lt;/code&gt; to &lt;code&gt;2.0.1&lt;/code&gt;. 
A major upgrade to &lt;code&gt;pandas&lt;/code&gt; at the same time a dataframe-based workload shows degrading performance? That's suspicious! &lt;/p&gt;
&lt;p&gt;To confirm this suspicion, we ran an A/B test based on the current Dask release (&lt;code&gt;2023.4.1&lt;/code&gt; at the time), testing the impact of the pandas upgrade. 
&lt;a href="https://github.com/coiled/benchmarks/actions/runs/4946428740"&gt;The results&lt;/a&gt; were clear: The runtime increased significantly with &lt;code&gt;pandas=2.0.1&lt;/code&gt; (sample).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/ab-test.png"&gt;&lt;/p&gt;
&lt;p&gt;Having shown that &lt;code&gt;pandas&lt;/code&gt; caused for the performance degradation and that we could reproduce it with the current Dask release, our release process for &lt;code&gt;2023.5.0&lt;/code&gt; &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1543878628"&gt;was cleared&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To further analyze the problem, we also derived a &lt;a href="https://matthewrocklin.com/minimal-bug-reports.html#minimal-complete-verifiable-examples"&gt;minimal local reproducer&lt;/a&gt; from the original workload:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from dask.distributed import Client

client = Client()
uri = &amp;quot;s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet&amp;quot;
ddf = dd.read_parquet(uri, engine=&amp;quot;pyarrow&amp;quot;, storage_options={&amp;quot;anon&amp;quot;: True}).persist()
wait(ddf)

ddf = ddf[[&amp;quot;id6&amp;quot;, &amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;, &amp;quot;v3&amp;quot;]]
(
    ddf[~ddf[&amp;quot;v3&amp;quot;].isna()][[&amp;quot;id6&amp;quot;, &amp;quot;v3&amp;quot;]]
    .groupby(&amp;quot;id6&amp;quot;, dropna=False, observed=True)
    .apply(
        lambda x: x.nlargest(2, columns=&amp;quot;v3&amp;quot;),
        meta={&amp;quot;id6&amp;quot;: &amp;quot;Int64&amp;quot;, &amp;quot;v3&amp;quot;: &amp;quot;float64&amp;quot;},
    )[[&amp;quot;v3&amp;quot;]]
).compute()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="investigating-the-pandas-performance-degradation"&gt;Investigating the pandas performance degradation&lt;/h2&gt;
&lt;p&gt;The only user-visible thing that changed between pandas 1.5.3 and pandas 2.0.1 was the default value
of &lt;code&gt;group_keys&lt;/code&gt; in &lt;code&gt;GroupBy&lt;/code&gt;. Switching to &lt;code&gt;group_keys=False&lt;/code&gt; with version 2.0.1
got us back to the initial runtime.
Now that we knew that pandas was to blame for the performance degradation, we had to create a 
reproducer in plain pandas to help fix the issue.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame(
    {
        &amp;quot;foo&amp;quot;: np.random.randint(1, 50_000, (100_000, )), 
        &amp;quot;bar&amp;quot;: np.random.randint(1, 100_000, (100_000, )),
    },
)

df.groupby(
    &amp;quot;foo&amp;quot;, group_keys=False
).apply(lambda x: x.nlargest(2, columns=&amp;quot;bar&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;group_keys=False&lt;/code&gt;: approx. 11 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;group_keys=True&lt;/code&gt;: approx. 15 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experimenting a bit showed us that the bottleneck got even worse while increasing the number of 
groups during the &lt;code&gt;groupby&lt;/code&gt; calculation. We settled on this version which is 30% slower with 
&lt;code&gt;group_keys=True&lt;/code&gt;, enough to be able to troubleshoot the problem. There was no obvious reason
why the changed value should bring a significant slowdown.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%prun&lt;/code&gt; showed us that the time was almost exclusively spent in a post-processing step that
combines all groups via &lt;code&gt;concat&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="addressing-the-performance-degradation"&gt;Addressing the performance degradation&lt;/h2&gt;
&lt;p&gt;Let's look at how both cases differ. The new version passes the grouping levels to &lt;code&gt;concat&lt;/code&gt;, which
are used to construct the resulting Index levels. This shouldn't be that slow though. Investigations
showed that this runs through a code-path that is very inefficient!&lt;/p&gt;
&lt;p&gt;Looking closer at the results of &lt;code&gt;%prun&lt;/code&gt; pointed us to one specific loop that took up most of
the runtime. This loop calculates the &lt;code&gt;codes&lt;/code&gt; for the resulting index based on the provided 
levels. It's slow, really slow! Every single element provided as &lt;code&gt;keys&lt;/code&gt;, which 
represent the number of groups, is checked against the whole level, which explains our previous 
observation that the runtime got worse with an increasing number of groups. You can check out the 
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#more-concatenating-with-group-keys"&gt;pandas user guide&lt;/a&gt;
if you are interested in situations where this is useful. Fortunately, we have a convenient
advantage in case of &lt;code&gt;groupby&lt;/code&gt;. We know beforehand that every key equals the specific level. 
We added a fast-path that exploits this knowledge getting the runtime of this step more or less to
zero.&lt;/p&gt;
&lt;p&gt;This change resulted in a &lt;a href="https://github.com/pandas-dev/pandas/pull/53195"&gt;small PR&lt;/a&gt; that cut 
the runtime of &lt;code&gt;group_keys=True&lt;/code&gt; to approximately 11 seconds as well.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Now that we made our pandas reproducer run 30% faster, we have to check whether we accomplished our
initial objective. Re-running the local Dask reproducer should give us an idea about the
impact on Dask. We got the performance down to 22 seconds as well! Promising news that saved
our plans for the evening!&lt;/p&gt;
&lt;p&gt;Unfortunately, we had to wait until pandas 2.0.2 was released to run a proper benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/benchmark_after.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This looks great! Our small pandas change translated to our Dask query and got performance back
to the previous level!&lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="performance"></category><category term="coiled"></category><category term="pandas"></category></entry><entry><title>Utilizing PyArrow to improve pandas and Dask workflows</title><link href="https://phofl.github.io/pyarrow-pandas-dask.html" rel="alternate"></link><published>2023-06-05T00:00:00+02:00</published><updated>2023-06-05T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-05:/pyarrow-pandas-dask.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/title.svg"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/title.svg"&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch of long-standing pains for users of both libraries. pandas users often complain to me that
pandas does not support missing values in arbitrary dtypes or that non-standard dtypes are not very
well supported. A particularly annoying problem for
Dask users is running out of memory with large datasets. PyArrow backed string columns 
consume up to 70% less memory compared to NumPy object columns and thus have the potential to 
mitigate this problem as well as providing a huge performance improvement.&lt;/p&gt;
&lt;p&gt;Support for PyArrow dtypes in pandas, and by extension Dask, is still relatively new. I would 
recommend caution when opting into the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; until at least pandas 2.1 is 
released. Not every part of both APIs is optimized yet. You should be able to get a big improvement 
in certain workflows though. This post will go over a couple of examples where I'd recommend switching to 
PyArrow right away, because it already provides huge benefits. &lt;/p&gt;
&lt;p&gt;Dask itself can benefit in various ways from PyArrow dtypes. We will investigate how PyArrow backed
strings can easily mitigate the pain point of running out of memory on Dask clusters and how we 
can improve performance through utilizing PyArrow.&lt;/p&gt;
&lt;p&gt;I am part of the pandas core team and was heavily involved in implementing and improving PyArrow 
support in pandas. I've recently joined 
&lt;a href="https://www.coiled.io?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; where I 
am working on Dask. One of my tasks is improving the PyArrow integration.&lt;/p&gt;
&lt;h2 id="general-overview-of-pyarrow-support"&gt;General overview of PyArrow support&lt;/h2&gt;
&lt;p&gt;PyArrow dtypes were initially introduced in pandas 1.5. The implementation was experimental and I
wouldn't recommend using it on pandas 1.5.x. Support for them is still relatively new. 
pandas 2.0 provides a huge improvement, including making opting into PyArrow backed DataFrames easy.
We are still working on supporting them properly everywhere, and thus they should be used with caution
until at least pandas 2.1 is released. Both projects work continuously to improve support throughout Dask and 
pandas.&lt;/p&gt;
&lt;p&gt;We encourage users to try them out! This will help us to get a better idea of what is still lacking
support or is not fast enough. Giving feedback helps us improve support and will drastically reduce the
time that is necessary to create a smooth user experience.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We will use the taxi dataset from New York City that contains all Uber and Lyft rides. It has
some interesting attributes like price, tips, driver pay and many more. The dataset can be found
&lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;here&lt;/a&gt; 
(see &lt;a href="https://www.nyc.gov/home/terms-of-use.page"&gt;terms of service&lt;/a&gt;) and is stored in parquet
files. When analyzing Dask queries, we will use a publicly available S3 bucket to simplify our
queries: &lt;code&gt;s3://coiled-datasets/uber-lyft-tlc/&lt;/code&gt;. We will use the dataset from December 2022 
for our pandas queries, since this is the maximum that fits comfortably into memory on my 
machine (24GB of RAM). We have to avoid stressing our RAM usage, since this might introduce side 
effects when analyzing performance.&lt;/p&gt;
&lt;p&gt;We will also investigate the performance of &lt;code&gt;read_csv&lt;/code&gt;. We will use the &lt;em&gt;Crimes in Chicago&lt;/em&gt; dataset
that can be found &lt;a href="https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="dask-cluster"&gt;Dask cluster&lt;/h2&gt;
&lt;p&gt;There are various different options to set up a Dask cluster, see the 
&lt;a href="https://docs.dask.org/en/stable/deploying.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask documentation&lt;/a&gt; for
a non-exhaustive list of deployment options. I will use
&lt;a href="https://docs.coiled.io/user_guide/index.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; to create a cluster on AWS with
30 machines through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

cluster = coiled.Cluster(
    n_workers=30,
    name=&amp;quot;dask-performance-comparisons&amp;quot;,
    region=&amp;quot;us-east-2&amp;quot;,  # this is the region of our dataset
    worker_vm_type=&amp;quot;m6i.large&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled is connected to my AWS account. It creates the cluster within my account and manages all
resources for me. 30 machines are enough to operate on our dataset comfortably. We will investigate
how we can reduce the required number of workers to 15 through some small 
modifications.&lt;/p&gt;
&lt;h2 id="pandas-stringdtype-backed-by-pyarrow"&gt;pandas StringDtype backed by PyArrow&lt;/h2&gt;
&lt;p&gt;We begin with a feature that was originally introduced over 3 years ago in pandas 1.0. Setting the
dtype in pandas or Dask to &lt;code&gt;string&lt;/code&gt; returns an object with &lt;code&gt;StringDtype&lt;/code&gt;. This feature is relatively mature and should
provide a smooth user experience.&lt;/p&gt;
&lt;p&gt;Historically, pandas represented string data through NumPy arrays with dtype &lt;code&gt;object&lt;/code&gt;. NumPy object data is stored as an 
array of pointers pointing to the actual data in memory. This makes iterating over an array containing 
strings very slow. pandas 1.0 initially introduced said
&lt;code&gt;StringDtype&lt;/code&gt; that allowed easier and consistent operations on strings. This dtype was still backed by Python 
strings and thus, wasn't very performant either. Rather, it provided a clear abstraction of string
data.&lt;/p&gt;
&lt;p&gt;pandas 1.3 finally introduced an enhancement to create an efficient string dtype. This datatype is backed by PyArrow arrays.
&lt;a href="https://arrow.apache.org/docs/python/index.html"&gt;PyArrow&lt;/a&gt; provides a data structure that enables 
performant and memory efficient string operations.
Starting from that point on, users could use a string dtype that was contiguous in memory and thus
very fast. This dtype can be requested through &lt;code&gt;string[pyarrow]&lt;/code&gt;. Alternatively, we can request it
by specifying &lt;code&gt;string&lt;/code&gt; as the dtype and setting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Dask builds on top of pandas, this string dtype is available here as well. On top of that, 
Dask offers a convenient option that automatically converts all string-data to &lt;code&gt;string[pyarrow]&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a convenient way of
avoiding NumPy object dtype for string columns. Additionally, it has the advantage that it
creates PyArrow arrays natively for I/O methods that operate with Arrow objects. 
On top of providing huge performance improvements, PyArrow strings consume significantly less
memory. An average Dask DataFrame with PyArrow strings consumes around 33-50% of the original
memory compared to NumPy object. This solves the biggest pain point for Dask users that is running
out of memory when operating on large datasets. The option enables global testing in Dask's test
suite. This ensures that PyArrow backed strings are mature enough to provide a smooth user
experience.&lt;/p&gt;
&lt;p&gt;Let's look at a few operations that represent typical string operations. We will start with a couple
of pandas examples before switching over to operations on our Dask cluster.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;df.convert_dtypes&lt;/code&gt; to convert our object columns to PyArrow string arrays. There
are more efficient ways of getting PyArrow dtypes in pandas that we will explore later. We will use
the Uber-Lyft dataset from December 2022, this file fits comfortably into memory on my machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;

df = pd.read_parquet(
    &amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;,
    columns=[
        &amp;quot;tips&amp;quot;, 
        &amp;quot;hvfhs_license_num&amp;quot;, 
        &amp;quot;driver_pay&amp;quot;, 
        &amp;quot;base_passenger_fare&amp;quot;, 
        &amp;quot;dispatching_base_num&amp;quot;,
    ],
)
df = df.convert_dtypes(
    convert_boolean=False, 
    convert_floating=False, 
    convert_integer=False,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our DataFrame has NumPy dtypes for all non-string columns in this example. Let's start with
filtering for all rides that were operated by Uber.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation creates a mask with True/False values that specify whether Uber operated a ride. 
This doesn't utilize any special string methods, but the equality comparison dispatches to 
PyArrow. Next, we will use the String accessor that is implemented in pandas and gives you access
to all kinds of string operations on a per-element basis. We want to find all rides that were
dispatched from a base starting with &lt;code&gt;"B028"&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;startswith&lt;/code&gt; iterates over our array and checks whether every string starts with the specified
substring. The advantage of PyArrow is easy to see. The data are contiguous in memory, which means
that we can efficiently iterate over them. Additionally, these arrays have a second array with 
pointers that point to the first memory address of every string, which makes computing the starting
sequence even faster.&lt;/p&gt;
&lt;p&gt;Finally, we look at a &lt;code&gt;GroupBy&lt;/code&gt; operation that groups over PyArrow string columns. The calculation
of the groups can dispatch to PyArrow as well, which is more efficient than factorizing
over a NumPy object array.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's look at how these operations stack up against DataFrames where string columns are represented
by NumPy object dtype.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/pandas_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;The results are more or less as we expected. The string based comparisons are significantly faster
when performed on PyArrow strings. Most string accessors should provide a huge performance 
improvement. Another interesting observation is memory usage, it is reduced by roughly 50% compared
to NumPy object dtype. We will take a closer look at this with Dask.&lt;/p&gt;
&lt;p&gt;Dask mirrors the pandas API and dispatches to pandas for most operations. Consequently, we can use
the same API to access PyArrow strings. A convenient option to request these globally is the option
mentioned above, which is what we will use here:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the biggest benefits of this option during development is that it enables easy testing of PyArrow
strings globally in Dask to make sure that everything works smoothly. We will utilize the Uber-Lyft
dataset for our explorations. The dataset takes up around 240GB of memory on our cluster. Our initial
cluster has 30 machines, which is enough to perform our computations comfortably.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask
import dask.dataframe as dd
from distributed import wait

dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)  # Wait till the computation is finished
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We persist the data in memory so that I/O performance does not influence our performance measurements. Our data is now
available in memory, which makes access fast. We will perform computations that are similar to our
pandas computations. One of the main goals is to show that the benefits from pandas will 
translate to computations in a distributed environment with Dask.&lt;/p&gt;
&lt;p&gt;One of the first observations is that the DataFrame with PyArrow backed string columns consumes only
130GB of memory, only half of what it consumed with NumPy object columns. We have only a few string
columns in our DataFrame, which means that the memory savings for string columns are actually higher than around 50%
when switching to PyArrow strings. Consequently, we will reduce the size of our cluster to 15 workers
when performing our operations on PyArrow string columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cluster.scale(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We measure the performance of the mask-operation and one of the String accessors together through
subsequent filtering of the DataFrame. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
df = df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we can use the same methods as in our previous example. This makes transitioning from
pandas to Dask relatively easy.&lt;/p&gt;
&lt;p&gt;Additionally, we will again compute a &lt;code&gt;GroupBy&lt;/code&gt; operation on our data. This is significantly harder
in a distributed environment, which makes the results more interesting. The previous operations
parallelize relatively easy onto a large cluster, while this is harder with &lt;code&gt;GroupBy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)

df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/Dask_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;We get nice improvements by factors of 2 and 3. This is especially intriguing since we reduced
the size of our cluster from 30 machines to 15, reducing the cost by 50%. Subsequently, we also reduced our computational 
resources by a factor of 2, which makes our performance improvement even more impressive. Thus,
the performance improved by a factor of 4 and 6 respectively. We can
perform the same computations on a smaller cluster, which saves money and is more efficient in general
and still get a performance boost out of it.&lt;/p&gt;
&lt;p&gt;Summarizing, we saw that PyArrow string-columns are a huge improvement to NumPy object columns in
DataFrames. Switching to PyArrow strings is a relatively small change that might improve the 
performance and efficiency of an average workflow that depends on string data. These improvements 
are visible in pandas and Dask!&lt;/p&gt;
&lt;h2 id="engine-keyword-in-io-methods"&gt;Engine keyword in I/O methods&lt;/h2&gt;
&lt;p&gt;We will now take a look at I/O functions in pandas and Dask. Some functions have custom implementations, 
like &lt;code&gt;read_csv&lt;/code&gt;, while others dispatch to another library, like &lt;code&gt;read_excel&lt;/code&gt; to 
&lt;code&gt;openpyxl&lt;/code&gt;. Some of these functions gained a new &lt;code&gt;engine&lt;/code&gt; keyword that enables us to dispatch to 
&lt;code&gt;PyArrow&lt;/code&gt;. The PyArrow parsers are multithreaded by default and hence, can provide a significant 
performance improvement.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This configuration will return the same results as the other engines. The only difference is that
PyArrow is used to read the data. The same option is available for &lt;code&gt;read_json&lt;/code&gt;.
The PyArrow-engines were added to provide a faster way of reading data. The improved speed is only
one of the advantages. The PyArrow parsers return the data as a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.Table.html"&gt;PyArrow Table&lt;/a&gt;. A PyArrow Table
provides built-in functionality to convert to a pandas &lt;code&gt;DataFrame&lt;/code&gt;. Depending on the data, this
might require a copy while casting to NumPy (string, integers with missing values, ...), which
brings an unnecessary slowdown. This is where the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; comes in.
It is implemented as an &lt;code&gt;ArrowExtensionArray&lt;/code&gt; class in pandas, which is backed by a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html"&gt;PyArrow ChunkedArray&lt;/a&gt;.
As a direct consequence, the conversion from a PyArrow Table to pandas is extremely cheap since it
does not require any copies. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;, dtype_backend=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a &lt;code&gt;DataFrame&lt;/code&gt; that is backed by PyArrow arrays. pandas isn't optimized everywhere
yet, so this can give you a slowdown in follow-up operations. It might be worth it if
the workload is particularly I/O heavy. Let's look at a direct comparison:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/pandas_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can see that PyArrow-engine and PyArrow dtypes provide a 15x speedup compared
to the C-engine.&lt;/p&gt;
&lt;p&gt;The same advantages apply to Dask. Dask wraps the pandas csv reader and
hence, gets the same features for free.&lt;/p&gt;
&lt;p&gt;The comparison for Dask is a bit more complicated. Firstly, my example reads the data from my local machine while
our Dask examples will read the data from a S3 bucket. Network speed will
be a relevant component. Also, distributed computations have some
overhead that we have to account for. &lt;/p&gt;
&lt;p&gt;We are purely looking for speed here, so we will read some timeseries data
from a public S3 bucket. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_csv(
    &amp;quot;s3://coiled-datasets/timeseries/20-years/csv/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
    engine=&amp;quot;pyarrow&amp;quot;,
    parse_dates=[&amp;quot;timestamp&amp;quot;],
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will execute this code-snippet for &lt;code&gt;engine="c"&lt;/code&gt;, &lt;code&gt;engine="pyarrow"&lt;/code&gt; and additionally
&lt;code&gt;engine="pyarrow"&lt;/code&gt; with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at some performance comparisons.
Both examples were executed with 30 machines on the cluster.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/Dask_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;The PyArrow-engine runs around 2 times as fast as the C-engine. Both implementations used the same
number of machines. The memory usage was reduced by 50% with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. The same
reduction is available if only object columns are converted to PyArrow strings, which gives
a better experience in follow-up operations.&lt;/p&gt;
&lt;p&gt;We've seen that the Arrow-engines provide significant speedups over the custom C implementations.
They don't support all features of the custom implementations yet, but if your use-case is 
compatible with the supported options, you should get a significant speedup for free.&lt;/p&gt;
&lt;p&gt;The case with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is a bit more complicated. Not all areas of the API are
optimized yet. If you spend a lot of time processing your data outside I/O functions, then this might not 
give you what you need. It will speed up your processing if your workflow spends a lot of
time reading the data.&lt;/p&gt;
&lt;h2 id="dtype_backend-in-pyarrow-native-io-readers"&gt;dtype_backend in PyArrow-native I/O readers&lt;/h2&gt;
&lt;p&gt;Some other I/O methods have an engine keyword as well. &lt;code&gt;read_parquet&lt;/code&gt; is the most popular 
example. The situation is a bit different here though. These I/O methods were already using the
PyArrow engine by default. So the parsing is as efficient as possible. One other potential
performance benefit is the usage of the &lt;code&gt;dtype_backend&lt;/code&gt; keyword. Normally, PyArrow will return
a PyArrow table which is then converted to a pandas DataFrame. The PyArrow dtypes are converted to
their NumPy equivalent. Setting &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt; avoids this conversion. This gives 
a decent performance improvement and saves a lot of memory.&lt;/p&gt;
&lt;p&gt;Let's look at one pandas performance comparison. We read the Uber-Lyft taxi data from December 2022.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_parquet(&amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the data with and without &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/pandas_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can easily see that the most time is taken up by the conversion after the reading of the
Parquet file was finished. The function runs 3 times as fast when avoiding
the conversion to NumPy dtypes.&lt;/p&gt;
&lt;p&gt;Dask has a specialized implementation for &lt;code&gt;read_parquet&lt;/code&gt; that has some advantages tailored to
distributed workloads compared to the pandas implementation. The common denominator is that both
functions dispatch to PyArrow to read the parquet file. Both have in common that the data are
converted to NumPy dtypes after successfully reading the file. We are reading
the whole Uber-Lyft dataset, which consumes around 240GB of memory on our
cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the dataset in 3 different configurations. First with the default NumPy dtypes, then with
the PyArrow string option turned on:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at what this means performance-wise:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/Dask_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;Similar to our pandas example, we can see that converting to NumPy dtypes takes up a huge chunk of
our runtime. The PyArrow dtypes give us a nice performance improvement. Both PyArrow configurations
use half of the memory that the NumPy dtypes are using.&lt;/p&gt;
&lt;p&gt;PyArrow-strings are a lot more mature than the general PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. Based on the 
performance chart we got, we get roughly the same performance improvement when using PyArrow 
strings and NumPy dtypes for all other dtypes. If a workflow does not work well enough on PyArrow 
dtypes yet, I'd recommend enabling PyArrow strings only.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen how we can leverage PyArrow in pandas in Dask right now. PyArrow backed string columns have the
potential to impact most workflows in a positive way and provide a smooth user experience with
pandas 2.0. Dask has a convenient option to globally avoid NumPy object dtype when possible, which
makes opting into PyArrow backed strings even easier. PyArrow also provides huge speedups in other
areas where available. The PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is still pretty new and has the 
potential to cut I/O times significantly right now. It is certainly worth exploring whether it can solve
performance bottlenecks. There is a lot of work going on to improve support for general PyArrow
dtypes with the potential to speed up an average workflow in the near future.&lt;/p&gt;
&lt;p&gt;There is a current proposal in pandas to start inferring strings as PyArrow backed strings by
default starting from pandas 3.0. Additionally, it includes many more areas where leaning more
onto PyArrow makes a lot of sense (e.g. Decimals, structured data, ...). You can read up on the
proposal &lt;a href="https://github.com/pandas-dev/pandas/pull/52711"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out to share your thoughts and feedback 
about PyArrow support in both libraries.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category><category term="dask"></category></entry></feed>