<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Hoefler - dask</title><link href="https://phofl.github.io/" rel="alternate"></link><link href="https://phofl.github.io/feeds/dask.atom.xml" rel="self"></link><id>https://phofl.github.io/</id><updated>2023-06-05T00:00:00+02:00</updated><entry><title>Utilizing PyArrow to improve pandas and Dask workflows</title><link href="https://phofl.github.io/pyarrow-pandas-dask.html" rel="alternate"></link><published>2023-06-05T00:00:00+02:00</published><updated>2023-06-05T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-05:/pyarrow-pandas-dask.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Get the most out of PyArrow support in pandas and Dask right now&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post investigates where we can use PyArrow to improve our pandas and Dask workflows right now.
General support for PyArrow dtypes was added with pandas 2.0 to &lt;a href="https://pandas.pydata.org"&gt;pandas&lt;/a&gt; 
and &lt;a href="https://www.dask.org?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask&lt;/a&gt;. This solves a
bunch of long-standing pains for users of both libraries. pandas users often complain to me that
pandas does not support missing values in arbitrary dtypes or that non-standard dtypes are not very
well supported. A particularly annoying problem for
Dask users is running out of memory with large datasets. PyArrow backed string columns 
consume up to 70% less memory compared to NumPy object columns and thus have the potential to 
mitigate this problem as well as providing a huge performance improvement.&lt;/p&gt;
&lt;p&gt;Support for PyArrow dtypes in pandas, and by extension Dask, is still relatively new. I would 
recommend caution when opting into the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; until at least pandas 2.1 is 
released. Not every part of both APIs is optimized yet. You should be able to get a big improvement 
in certain workflows though. This post will go over a couple of examples where I'd recommend switching to 
PyArrow right away, because it already provides huge benefits. &lt;/p&gt;
&lt;p&gt;Dask itself can benefit in various ways from PyArrow dtypes. We will investigate how PyArrow backed
strings can easily mitigate the pain point of running out of memory on Dask clusters and how we 
can improve performance through utilizing PyArrow.&lt;/p&gt;
&lt;p&gt;I am part of the pandas core team and was heavily involved in implementing and improving PyArrow 
support in pandas. I've recently joined 
&lt;a href="https://www.coiled.io?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; where I 
am working on Dask. One of my tasks is improving the PyArrow integration.&lt;/p&gt;
&lt;h2 id="general-overview-of-pyarrow-support"&gt;General overview of PyArrow support&lt;/h2&gt;
&lt;p&gt;PyArrow dtypes were initially introduced in pandas 1.5. The implementation was experimental and I
wouldn't recommend using it on pandas 1.5.x. Support for them is still relatively new. 
pandas 2.0 provides a huge improvement, including making opting into PyArrow backed DataFrames easy.
We are still working on supporting them properly everywhere, and thus they should be used with caution
until at least pandas 2.1 is released. Both projects work continuously to improve support throughout Dask and 
pandas.&lt;/p&gt;
&lt;p&gt;We encourage users to try them out! This will help us to get a better idea of what is still lacking
support or is not fast enough. Giving feedback helps us improve support and will drastically reduce the
time that is necessary to create a smooth user experience.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We will use the taxi dataset from New York City that contains all Uber and Lyft rides. It has
some interesting attributes like price, tips, driver pay and many more. The dataset can be found
&lt;a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"&gt;here&lt;/a&gt; 
(see &lt;a href="https://www.nyc.gov/home/terms-of-use.page"&gt;terms of service&lt;/a&gt;) and is stored in parquet
files. When analyzing Dask queries, we will use a publicly available S3 bucket to simplify our
queries: &lt;code&gt;s3://coiled-datasets/uber-lyft-tlc/&lt;/code&gt;. We will use the dataset from December 2022 
for our pandas queries, since this is the maximum that fits comfortably into memory on my 
machine (24GB of RAM). We have to avoid stressing our RAM usage, since this might introduce side 
effects when analyzing performance.&lt;/p&gt;
&lt;p&gt;We will also investigate the performance of &lt;code&gt;read_csv&lt;/code&gt;. We will use the &lt;em&gt;Crimes in Chicago&lt;/em&gt; dataset
that can be found &lt;a href="https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="dask-cluster"&gt;Dask cluster&lt;/h2&gt;
&lt;p&gt;There are various different options to set up a Dask cluster, see the 
&lt;a href="https://docs.dask.org/en/stable/deploying.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Dask documentation&lt;/a&gt; for
a non-exhaustive list of deployment options. I will use
&lt;a href="https://docs.coiled.io/user_guide/index.html?utm_source=phofl&amp;amp;utm_medium=pyarrow-in-pandas-and-dask"&gt;Coiled&lt;/a&gt; to create a cluster on AWS with
30 machines through:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

cluster = coiled.Cluster(
    n_workers=30,
    name=&amp;quot;dask-performance-comparisons&amp;quot;,
    region=&amp;quot;us-east-2&amp;quot;,  # this is the region of our dataset
    worker_vm_type=&amp;quot;m6i.large&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled is connected to my AWS account. It creates the cluster within my account and manages all
resources for me. 30 machines are enough to operate on our dataset comfortably. We will investigate
how we can reduce the required number of workers to 15 through some small 
modifications.&lt;/p&gt;
&lt;h2 id="pandas-stringdtype-backed-by-pyarrow"&gt;pandas StringDtype backed by PyArrow&lt;/h2&gt;
&lt;p&gt;We begin with a feature that was originally introduced over 3 years ago in pandas 1.0. Setting the
dtype in pandas or Dask to &lt;code&gt;string&lt;/code&gt; returns an object with &lt;code&gt;StringDtype&lt;/code&gt;. This feature is relatively mature and should
provide a smooth user experience.&lt;/p&gt;
&lt;p&gt;Historically, pandas represented string data through NumPy arrays with dtype &lt;code&gt;object&lt;/code&gt;. NumPy object data is stored as an 
array of pointers pointing to the actual data in memory. This makes iterating over an array containing 
strings very slow. pandas 1.0 initially introduced said
&lt;code&gt;StringDtype&lt;/code&gt; that allowed easier and consistent operations on strings. This dtype was still backed by Python 
strings and thus, wasn't very performant either. Rather, it provided a clear abstraction of string
data.&lt;/p&gt;
&lt;p&gt;pandas 1.3 finally introduced an enhancement to create an efficient string dtype. This datatype is backed by PyArrow arrays.
&lt;a href="https://arrow.apache.org/docs/python/index.html"&gt;PyArrow&lt;/a&gt; provides a data structure that enables 
performant and memory efficient string operations.
Starting from that point on, users could use a string dtype that was contiguous in memory and thus
very fast. This dtype can be requested through &lt;code&gt;string[pyarrow]&lt;/code&gt;. Alternatively, we can request it
by specifying &lt;code&gt;string&lt;/code&gt; as the dtype and setting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Dask builds on top of pandas, this string dtype is available here as well. On top of that, 
Dask offers a convenient option that automatically converts all string-data to &lt;code&gt;string[pyarrow]&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a convenient way of
avoiding NumPy object dtype for string columns. Additionally, it has the advantage that it
creates PyArrow arrays natively for I/O methods that operate with Arrow objects. 
On top of providing huge performance improvements, PyArrow strings consume significantly less
memory. An average Dask DataFrame with PyArrow strings consumes around 33-50% of the original
memory compared to NumPy object. This solves the biggest pain point for Dask users that is running
out of memory when operating on large datasets. The option enables global testing in Dask's test
suite. This ensures that PyArrow backed strings are mature enough to provide a smooth user
experience.&lt;/p&gt;
&lt;p&gt;Let's look at a few operations that represent typical string operations. We will start with a couple
of pandas examples before switching over to operations on our Dask cluster.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;df.convert_dtypes&lt;/code&gt; to convert our object columns to PyArrow string arrays. There
are more efficient ways of getting PyArrow dtypes in pandas that we will explore later. We will use
the Uber-Lyft dataset from December 2022, this file fits comfortably into memory on my machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

pd.options.mode.string_storage = &amp;quot;pyarrow&amp;quot;

df = pd.read_parquet(
    &amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;,
    columns=[
        &amp;quot;tips&amp;quot;, 
        &amp;quot;hvfhs_license_num&amp;quot;, 
        &amp;quot;driver_pay&amp;quot;, 
        &amp;quot;base_passenger_fare&amp;quot;, 
        &amp;quot;dispatching_base_num&amp;quot;,
    ],
)
df = df.convert_dtypes(
    convert_boolean=False, 
    convert_floating=False, 
    convert_integer=False,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our DataFrame has NumPy dtypes for all non-string columns in this example. Let's start with
filtering for all rides that were operated by Uber.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation creates a mask with True/False values that specify whether Uber operated a ride. 
This doesn't utilize any special string methods, but the equality comparison dispatches to 
PyArrow. Next, we will use the String accessor that is implemented in pandas and gives you access
to all kinds of string operations on a per-element basis. We want to find all rides that were
dispatched from a base starting with &lt;code&gt;"B028"&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;startswith&lt;/code&gt; iterates over our array and checks whether every string starts with the specified
substring. The advantage of PyArrow is easy to see. The data are contiguous in memory, which means
that we can efficiently iterate over them. Additionally, these arrays have a second array with 
pointers that point to the first memory address of every string, which makes computing the starting
sequence even faster.&lt;/p&gt;
&lt;p&gt;Finally, we look at a &lt;code&gt;GroupBy&lt;/code&gt; operation that groups over PyArrow string columns. The calculation
of the groups can dispatch to PyArrow as well, which is more efficient than factorizing
over a NumPy object array.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's look at how these operations stack up against DataFrames where string columns are represented
by NumPy object dtype.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/pandas_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;The results are more or less as we expected. The string based comparisons are significantly faster
when performed on PyArrow strings. Most string accessors should provide a huge performance 
improvement. Another interesting observation is memory usage, it is reduced by roughly 50% compared
to NumPy object dtype. We will take a closer look at this with Dask.&lt;/p&gt;
&lt;p&gt;Dask mirrors the pandas API and dispatches to pandas for most operations. Consequently, we can use
the same API to access PyArrow strings. A convenient option to request these globally is the option
mentioned above, which is what we will use here:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the biggest benefits of this option during development is that it enables easy testing of PyArrow
strings globally in Dask to make sure that everything works smoothly. We will utilize the Uber-Lyft
dataset for our explorations. The dataset takes up around 240GB of memory on our cluster. Our initial
cluster has 30 machines, which is enough to perform our computations comfortably.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask
import dask.dataframe as dd
from distributed import wait

dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)  # Wait till the computation is finished
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We persist the data in memory so that I/O performance does not influence our performance measurements. Our data is now
available in memory, which makes access fast. We will perform computations that are similar to our
pandas computations. One of the main goals is to show that the benefits from pandas will 
translate to computations in a distributed environment with Dask.&lt;/p&gt;
&lt;p&gt;One of the first observations is that the DataFrame with PyArrow backed string columns consumes only
130GB of memory, only half of what it consumed with NumPy object columns. We have only a few string
columns in our DataFrame, which means that the memory savings for string columns are actually higher than around 50%
when switching to PyArrow strings. Consequently, we will reduce the size of our cluster to 15 workers
when performing our operations on PyArrow string columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cluster.scale(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We measure the performance of the mask-operation and one of the String accessors together through
subsequent filtering of the DataFrame. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df[df[&amp;quot;hvfhs_license_num&amp;quot;] == &amp;quot;HV0003&amp;quot;]
df = df[df[&amp;quot;dispatching_base_num&amp;quot;].str.startswith(&amp;quot;B028&amp;quot;)]
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we can use the same methods as in our previous example. This makes transitioning from
pandas to Dask relatively easy.&lt;/p&gt;
&lt;p&gt;Additionally, we will again compute a &lt;code&gt;GroupBy&lt;/code&gt; operation on our data. This is significantly harder
in a distributed environment, which makes the results more interesting. The previous operations
parallelize relatively easy onto a large cluster, while this is harder with &lt;code&gt;GroupBy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df.groupby(
    [&amp;quot;hvfhs_license_num&amp;quot;, &amp;quot;dispatching_base_num&amp;quot;]
).mean(numeric_only=True)

df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="" src=".././images/arrow_backend/Dask_string_performance_comparison.svg"&gt;&lt;/p&gt;
&lt;p&gt;We get nice improvements by factors of 2 and 3. This is especially intriguing since we reduced
the size of our cluster from 30 machines to 15, reducing the cost by 50%. Subsequently, we also reduced our computational 
resources by a factor of 2, which makes our performance improvement even more impressive. Thus,
the performance improved by a factor of 4 and 6 respectively. We can
perform the same computations on a smaller cluster, which saves money and is more efficient in general,
and still get a performance boost out of it.&lt;/p&gt;
&lt;p&gt;Summarizing, we saw that PyArrow string-columns are a huge improvement when comparing them to NumPy object columns in
DataFrames. Switching to PyArrow strings is a relatively small change that might improve the 
performance and efficiency of an average workflow that depends on string data immensely. These improvements 
are equally visible in pandas and Dask!&lt;/p&gt;
&lt;h2 id="engine-keyword-in-io-methods"&gt;Engine keyword in I/O methods&lt;/h2&gt;
&lt;p&gt;We will now take a look at I/O functions in pandas and Dask. Some functions have custom implementations, 
like &lt;code&gt;read_csv&lt;/code&gt;, while others dispatch to another library, like &lt;code&gt;read_excel&lt;/code&gt; to 
&lt;code&gt;openpyxl&lt;/code&gt;. Some of these functions gained a new &lt;code&gt;engine&lt;/code&gt; keyword that enables us to dispatch to 
&lt;code&gt;PyArrow&lt;/code&gt;. The PyArrow parsers are multithreaded by default and hence, can provide a significant 
performance improvement.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This configuration will return the same results as the other engines. The only difference is that
PyArrow is used to read the data. The same option is available for &lt;code&gt;read_json&lt;/code&gt;.
The PyArrow-engines were added to provide a faster way of reading data. The improved speed is only
one of the advantages. The PyArrow parsers return the data as a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.Table.html"&gt;PyArrow Table&lt;/a&gt;. A PyArrow Table
provides built-in functionality to convert to a pandas &lt;code&gt;DataFrame&lt;/code&gt;. Depending on the data, this
might require a copy while casting to NumPy (string, integers with missing values, ...), which
brings an unnecessary slowdown. This is where the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; comes in.
It is implemented as an &lt;code&gt;ArrowExtensionArray&lt;/code&gt; class in pandas, which is backed by a 
&lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html"&gt;PyArrow ChunkedArray&lt;/a&gt;.
As a direct consequence, the conversion from a PyArrow Table to pandas is extremely cheap since it
does not require any copies. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_csv(&amp;quot;Crimes_-_2001_to_Present.csv&amp;quot;, engine=&amp;quot;pyarrow&amp;quot;, dtype_backend=&amp;quot;pyarrow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a &lt;code&gt;DataFrame&lt;/code&gt; that is backed by PyArrow arrays. pandas isn't optimized everywhere
yet, so this can give you a slowdown in follow-up operations. It might be worth it if
the workload is particularly I/O heavy. Let's look at a direct comparison:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/pandas_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can see that PyArrow-engine and PyArrow dtypes provide a 15x speedup compared
to the C-engine.&lt;/p&gt;
&lt;p&gt;The same advantages apply to Dask. Dask wraps the pandas csv reader and
hence, gets the same features for free.&lt;/p&gt;
&lt;p&gt;The comparison for Dask is a bit more complicated. Firstly, my example reads the data from my local machine while
our Dask examples will read the data from a S3 bucket. Network speed will
be a relevant component. Also, distributed computations have some
overhead that we have to account for. &lt;/p&gt;
&lt;p&gt;We are purely looking for speed here, so we will read some timeseries data
from a public S3 bucket. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_csv(
    &amp;quot;s3://coiled-datasets/timeseries/20-years/csv/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
    engine=&amp;quot;pyarrow&amp;quot;,
    parse_dates=[&amp;quot;timestamp&amp;quot;],
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will execute this code-snippet for &lt;code&gt;engine="c"&lt;/code&gt;, &lt;code&gt;engine="pyarrow"&lt;/code&gt; and additionally
&lt;code&gt;engine="pyarrow"&lt;/code&gt; with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at some performance comparisons.
Both examples were executed with 30 machines on the cluster.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./../images/arrow_backend/Dask_read_csv_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;The PyArrow-engine runs around 2 times as fast as the C-engine. Both implementations used the same
number of machines. The memory usage was reduced by 50% with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. The same
reduction is available if only object columns are converted to PyArrow strings, which gives
a better experience in follow-up operations.&lt;/p&gt;
&lt;p&gt;We've seen that the Arrow-engines provide significant speedups over the custom C implementations.
They don't support all features of the custom implementations yet, but if your use-case is 
compatible with the supported options, you should get a significant speedup for free.&lt;/p&gt;
&lt;p&gt;The case with the PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is a bit more complicated. Not all areas of the API are
optimized yet. If you spend a lot of time processing your data outside I/O functions, then this might not 
give you what you need. It will speed up your processing if your workflow spends a lot of
time reading the data.&lt;/p&gt;
&lt;h2 id="dtype_backend-in-pyarrow-native-io-readers"&gt;dtype_backend in PyArrow-native I/O readers&lt;/h2&gt;
&lt;p&gt;Some other I/O methods have an engine keyword as well. &lt;code&gt;read_parquet&lt;/code&gt; is the most popular 
example. The situation is a bit different here though. These I/O methods were already using the
PyArrow engine by default. So the parsing is as efficient as possible. One other potential
performance benefit is the usage of the &lt;code&gt;dtype_backend&lt;/code&gt; keyword. Normally, PyArrow will return
a PyArrow table which is then converted to a pandas DataFrame. The PyArrow dtypes are converted to
their NumPy equivalent. Setting &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt; avoids this conversion. This gives 
a decent performance improvement and saves a lot of memory.&lt;/p&gt;
&lt;p&gt;Let's look at one pandas performance comparison. We read the Uber-Lyft taxi data from December 2022.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.read_parquet(&amp;quot;fhvhv_tripdata_2022-10.parquet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the data with and without &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/pandas_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;We can easily see that the most time is taken up by the conversion after the reading of the
Parquet file was finished. The function runs 3 times as fast when avoiding
the conversion to NumPy dtypes.&lt;/p&gt;
&lt;p&gt;Dask has a specialized implementation for &lt;code&gt;read_parquet&lt;/code&gt; that has some advantages tailored to
distributed workloads compared to the pandas implementation. The common denominator is that both
functions dispatch to PyArrow to read the parquet file. Both have in common that the data are
converted to NumPy dtypes after successfully reading the file. We are reading
the whole Uber-Lyft dataset, which consumes around 240GB of memory on our
cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import dask.dataframe as dd
from distributed import wait

df = dd.read_parquet(
    &amp;quot;s3://coiled-datasets/uber-lyft-tlc/&amp;quot;,
    storage_options={&amp;quot;anon&amp;quot;: True},
)
df = df.persist()
wait(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the dataset in 3 different configurations. First with the default NumPy dtypes, then with
the PyArrow string option turned on:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;dask.config.set({&amp;quot;dataframe.convert-string&amp;quot;: True})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly with &lt;code&gt;dtype_backend="pyarrow"&lt;/code&gt;. Let's look at what this means performance-wise:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/arrow_backend/Dask_read_parquet_performance.svg"&gt;&lt;/p&gt;
&lt;p&gt;Similar to our pandas example, we can see that converting to NumPy dtypes takes up a huge chunk of
our runtime. The PyArrow dtypes give us a nice performance improvement. Both PyArrow configurations
use half of the memory that the NumPy dtypes are using.&lt;/p&gt;
&lt;p&gt;PyArrow-strings are a lot more mature than the general PyArrow &lt;code&gt;dtype_backend&lt;/code&gt;. Based on the 
performance chart we got, we get roughly the same performance improvement when using PyArrow 
strings and NumPy dtypes for all other dtypes. If a workflow does not work well enough on PyArrow 
dtypes yet, I'd recommend enabling PyArrow strings only.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen how we can leverage PyArrow in pandas in Dask right now. PyArrow backed string columns have the
potential to impact most workflows in a positive way and provide a smooth user experience with
pandas 2.0. Dask has a convenient option to globally avoid NumPy object dtype when possible, which
makes opting into PyArrow backed strings even easier. PyArrow also provides huge speedups in other
areas where available. The PyArrow &lt;code&gt;dtype_backend&lt;/code&gt; is still pretty new and has the 
potential to cut I/O times significantly right now. It is certainly worth exploring whether it can solve
performance bottlenecks. There is a lot of work going on to improve support for general PyArrow
dtypes with the potential to speed up an average workflow in the near future.&lt;/p&gt;
&lt;p&gt;There is a current proposal in pandas to start inferring strings as PyArrow backed strings by
default starting from pandas 3.0. Additionally, it includes many more areas where leaning more
onto PyArrow makes a lot of sense (e.g. Decimals, structured data, ...). You can read up on the
proposal &lt;a href="https://github.com/pandas-dev/pandas/pull/52711"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading. Feel free to reach out to share your thoughts and feedback 
about PyArrow support in both libraries.&lt;/p&gt;</content><category term="posts"></category><category term="pandas"></category><category term="dask"></category></entry></feed>