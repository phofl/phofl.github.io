<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Hoefler - [coiled</title><link href="https://phofl.github.io/" rel="alternate"></link><link href="https://phofl.github.io/feeds/coiled.atom.xml" rel="self"></link><id>https://phofl.github.io/</id><updated>2023-07-24T00:00:00+02:00</updated><entry><title>How to Train a Neural Network on a GPU in the Cloud with ``coiled functions``</title><link href="https://phofl.github.io/coiled-functions-pytorch.html" rel="alternate"></link><published>2023-07-24T00:00:00+02:00</published><updated>2023-07-24T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-07-24:/coiled-functions-pytorch.html</id><summary type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="" src="../images/coiled_run/coiled-run-pytorch.png"&gt;&lt;/p&gt;
&lt;p&gt;We recently pushed out two new and experimental features &lt;a href="https://docs.coiled.io/user_guide/labs/jobs.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled jobs&lt;/a&gt;
and &lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;coiled functions&lt;/a&gt;
which is a deviation of &lt;code&gt;coiled jobs&lt;/code&gt;. We are excited about both of them because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to scale up any given program on any hardware in the cloud.&lt;/li&gt;
&lt;li&gt;Make GPUs easily accessible without going through the pains of setting up environments in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post will provide an example how to utilize &lt;code&gt;coiled functions&lt;/code&gt; to seamlessly train a 
&lt;strong&gt;neural network&lt;/strong&gt; on a GPU that is hosted in the cloud.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;We have to start with creating a model on our local machine before we can start worrying about
training it. This blog post is not dedicated to figuring out a fancy mode, we will utilize the
&lt;code&gt;Net&lt;/code&gt; model that is given in the &lt;a href="https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html#training-your-pytorch-model"&gt;PyTorch tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can simply add the model definition to our python script. There is no need to do anything
different. Similarly, we will use the transformer that is given there as well.&lt;/p&gt;
&lt;p&gt;The next step is creating a function that we can use to train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train(transform):
    device = torch.device(&amp;quot;cpu&amp;quot;)
    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now train our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will train our model on the CPU of our local machine. The training is reasonably quick for such
a small model, but training time will grow exponentially as our model gets larger or if we are
using a significantly bigger dataset. Training the model on the CPU won't be sufficient anymore.
Additionally, there are a lot of machines out there that don't have GPUs built into them. For example, I'm using a MacBook Pro with an M2 CPU, which means my machine doesn't support &lt;code&gt;cuda&lt;/code&gt;.
Consequently, we need a different solution to make these steps accessible for folks who don't have
access to a local GPU.&lt;/p&gt;
&lt;h2 id="using-coiled-functions-to-train-the-model-on-a-cloud-hosted-gpu"&gt;Using &lt;code&gt;coiled functions&lt;/code&gt; to train the model on a cloud-hosted GPU&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.coiled.io/user_guide/labs/run.html?utm_source=phofl&amp;amp;utm_medium=coiled-functions-gpu"&gt;Coiled functions&lt;/a&gt; come into the equation if you 
need access to resources that aren't available
locally. Coiled can connect to AWS or GCP and thus, use all resources that are available there.
We will go through the necessary steps to train our model on a GPU that is hosted on AWS instead
of our local CPU. &lt;/p&gt;
&lt;p&gt;The first step includes defining a Python environment to run our computations. We simply include
PyTorch, CUDA, and Coiled, that's it. Generally, you should use the same Python version that is
installed locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import coiled

coiled.create_software_environment(
    name=&amp;quot;pytorch&amp;quot;,
    conda={
        &amp;quot;channels&amp;quot;: [&amp;quot;pytorch&amp;quot;, &amp;quot;nvidia&amp;quot;, &amp;quot;conda-forge&amp;quot;, &amp;quot;defaults&amp;quot;],
        &amp;quot;dependencies&amp;quot;: [
            &amp;quot;python=3.11&amp;quot;,
            &amp;quot;coiled&amp;quot;,
            &amp;quot;pytorch&amp;quot;,
            &amp;quot;torchvision&amp;quot;,
            &amp;quot;torchaudio&amp;quot;,
            &amp;quot;cudatoolkit&amp;quot;,
            &amp;quot;pynvml&amp;quot;,
        ],
    },
    gpu_enabled=True,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coiled will create a Python environment for you. This step is only necessary when running your 
script for the first time. The resulting environment is cached which makes further runs more
efficient.&lt;/p&gt;
&lt;p&gt;The next step is adding the &lt;code&gt;@coiled.run&lt;/code&gt; decorator to our training
function that tells our program we want to execute said function on a machine in the cloud.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;, # GPU instance type
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, we have to tell PyTorch that we want to train the model on the GPU.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def train():
    import torch
    # tell PyTorch to use the GPU
    device = torch.device(&amp;quot;cuda:0&amp;quot;)
    ...
    return net.to(torch.device(&amp;quot;cpu&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting this all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@coiled.run(
    worker_vm_type=&amp;quot;g5.xlarge&amp;quot;,
    region=&amp;quot;us-west-2&amp;quot;,
    software=&amp;quot;pytorch&amp;quot;,
)
def train(transform):
    import torch
    device = torch.device(&amp;quot;cuda:0&amp;quot;)

    net = Net()
    net = net.to(device)

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform,
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=4, shuffle=True, num_workers=2,
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    return net.to(torch.device(&amp;quot;cpu&amp;quot;))


if __name__ == &amp;quot;__main__&amp;quot;:
    train()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's take a brief look at the arguments to &lt;code&gt;coiled.run()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;worker_vm_type&lt;/code&gt;: This specifies the type of &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;EC2 instance&lt;/a&gt;.
  We are looking for an instance that has a GPU attached to it. The G5 family has Nvidia GPUs
  attached to it. The smallest version is sufficient for our example, but you can choose instances
  with up to 8 GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;region&lt;/code&gt;: The region specifies the AWS region that our VM is started in. We observed that &lt;code&gt;"us-west-2"&lt;/code&gt;
  is a region where GPUs are easier to get. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;software&lt;/code&gt;: This specifies the coiled software environment that is installed. This corresponds
  to the environment that we previously created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;coiled.run()&lt;/code&gt; will now start a VM in AWS with the specified EC2 instance. The VM is normally up
and running in 1-2 minutes. The previously specified Python environment is installed automatically.
Coiled executes the function on said VM. Inputs of your function are serialized and sent to the VM
as well. It makes sense to download the training data on the VM to reduce time that is spent sending
data to AWS. The function returns our model back to our local machine so that we can use it locally
without depending on AWS.&lt;/p&gt;
&lt;p&gt;Coiled will shut down the VM immediately after the Python interpreter finishes. This is mostly to
reduce costs. You can specify a certain amount of time that the VM is kept alive through 
&lt;code&gt;keepalive="5 minutes"&lt;/code&gt;. This ensures that new local runs can connect to the same VM avoiding
the boot time of up to 2 minutes; we call this a warm start.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;coiled functions&lt;/code&gt; enables you to seamlessly port the training process for a neural network
from your local machine to AWS or GCP. This grants everyone access to multiple GPUs or huge
machines independent of the local machine that is actually used. Training a neural network on a
GPU becomes as easy as adding a decorator to the training function.&lt;/p&gt;</content><category term="posts"></category><category term="[coiled"></category><category term="jobs"></category><category term="gpu"></category><category term="machine learning]"></category></entry><entry><title>Dask performance benchmarking put to the test: Fixing a pandas bottleneck</title><link href="https://phofl.github.io/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html" rel="alternate"></link><published>2023-06-28T00:00:00+02:00</published><updated>2023-06-28T00:00:00+02:00</updated><author><name>Patrick Hoefler</name></author><id>tag:phofl.github.io,2023-06-28:/dask-performance-benchmarking-put-to-the-test-fixing-a-pandas-bottleneck.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Getting notified of a significant performance regression the day before release sucks, but quickly identifying and resolving it feels great!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We were getting set up at our booth at JupyterCon 2023 when we received a notification:
An engineer on our team had spotted a significant performance regression in Dask.
With an impact of 40% increased runtime, it blocked the release planned for the next day!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/performance-regression.png"&gt;&lt;/p&gt;
&lt;p&gt;Luckily, the other attendees still focused on coffee and breakfast, so we commandeered an abandoned table next to our booth and got to work.&lt;/p&gt;
&lt;h2 id="performance-testing-at-coiled"&gt;Performance testing at Coiled&lt;/h2&gt;
&lt;p&gt;The performance problem &lt;a href="https://github.com/coiled/benchmarks/issues/840"&gt;had been flagged&lt;/a&gt; by the automated performance testing for Dask that we developed at &lt;a href="https://www.coiled.io/?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Coiled&lt;/a&gt;.
If you have not read Guido Imperiale's &lt;a href="https://blog.coiled.io/blog/performance-testing.html?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;blog post&lt;/a&gt; on our approach to performance testing, here is a summary:
With &lt;a href="https://github.com/coiled/benchmarks"&gt;&lt;code&gt;coiled/benchmarks&lt;/code&gt;&lt;/a&gt;, we created a benchmark suite that contains a variety of common workloads and operations with Dask, including standardized ones like the &lt;a href="https://github.com/h2oai/db-benchmark"&gt;&lt;code&gt;h2oai/db-benchmark&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It also contains tooling that allows us to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatically &lt;a href="https://blog.coiled.io/blog/performance-testing.html#nightly-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;detect performance regressions&lt;/a&gt; in Dask and raise them as issues.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.coiled.io/blog/performance-testing.html#a-b-tests?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;Run A/B tests&lt;/a&gt; to assess the performance impact of different versions of Dask, upstream packages, or cluster configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the former started this journey, the latter will also come in handy soon.  &lt;/p&gt;
&lt;h2 id="identifying-the-problem"&gt;Identifying the problem&lt;/h2&gt;
&lt;p&gt;Our automated regression testing had alerted us that &lt;a href="https://github.com/coiled/benchmarks/blob/895a13db09eb3172155e7b1260a5698f2284f5b7/tests/benchmarks/test_h2o.py#L140-L151"&gt;&lt;code&gt;test_h2o.py::test_q8&lt;/code&gt;&lt;/a&gt; had experienced &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1542560550"&gt;a significant increase&lt;/a&gt; in runtime across all data sizes and file formats. 
From the &lt;a href="https://benchmarks.coiled.io?utm_source=phofl&amp;amp;utm_medium=dask-benchmark-pandas-bottleneck"&gt;historical report&lt;/a&gt; of our benchmarking suite, we could see that &lt;code&gt;dask/dask&lt;/code&gt; and &lt;code&gt;dask/distributed&lt;/code&gt; were unlikely to be the culprit: 
Nothing had changed on &lt;code&gt;dask/dask&lt;/code&gt; when the performance started to degrade, and there was only one unrelated change on &lt;code&gt;dask/distributed&lt;/code&gt;. 
That left us with the Coiled platform and upstream packages as possible candidates. &lt;/p&gt;
&lt;p&gt;After digging deeper into the cluster data, we noticed that &lt;code&gt;pandas&lt;/code&gt; had been upgraded from &lt;code&gt;1.5.3&lt;/code&gt; to &lt;code&gt;2.0.1&lt;/code&gt;. 
A major upgrade to &lt;code&gt;pandas&lt;/code&gt; at the same time a dataframe-based workload shows degrading performance? That's suspicious! &lt;/p&gt;
&lt;p&gt;To confirm this suspicion, we ran an A/B test based on the current Dask release (&lt;code&gt;2023.4.1&lt;/code&gt; at the time), testing the impact of the pandas upgrade. 
&lt;a href="https://github.com/coiled/benchmarks/actions/runs/4946428740"&gt;The results&lt;/a&gt; were clear: The runtime increased significantly with &lt;code&gt;pandas=2.0.1&lt;/code&gt; (sample).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/ab-test.png"&gt;&lt;/p&gt;
&lt;p&gt;Having shown that &lt;code&gt;pandas&lt;/code&gt; caused for the performance degradation and that we could reproduce it with the current Dask release, our release process for &lt;code&gt;2023.5.0&lt;/code&gt; &lt;a href="https://github.com/dask/community/issues/322#issuecomment-1543878628"&gt;was cleared&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To further analyze the problem, we also derived a &lt;a href="https://matthewrocklin.com/minimal-bug-reports.html#minimal-complete-verifiable-examples"&gt;minimal local reproducer&lt;/a&gt; from the original workload:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from dask.distributed import Client

client = Client()
uri = &amp;quot;s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet&amp;quot;
ddf = dd.read_parquet(uri, engine=&amp;quot;pyarrow&amp;quot;, storage_options={&amp;quot;anon&amp;quot;: True}).persist()
wait(ddf)

ddf = ddf[[&amp;quot;id6&amp;quot;, &amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;, &amp;quot;v3&amp;quot;]]
(
    ddf[~ddf[&amp;quot;v3&amp;quot;].isna()][[&amp;quot;id6&amp;quot;, &amp;quot;v3&amp;quot;]]
    .groupby(&amp;quot;id6&amp;quot;, dropna=False, observed=True)
    .apply(
        lambda x: x.nlargest(2, columns=&amp;quot;v3&amp;quot;),
        meta={&amp;quot;id6&amp;quot;: &amp;quot;Int64&amp;quot;, &amp;quot;v3&amp;quot;: &amp;quot;float64&amp;quot;},
    )[[&amp;quot;v3&amp;quot;]]
).compute()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="investigating-the-pandas-performance-degradation"&gt;Investigating the pandas performance degradation&lt;/h2&gt;
&lt;p&gt;The only user-visible thing that changed between pandas 1.5.3 and pandas 2.0.1 was the default value
of &lt;code&gt;group_keys&lt;/code&gt; in &lt;code&gt;GroupBy&lt;/code&gt;. Switching to &lt;code&gt;group_keys=False&lt;/code&gt; with version 2.0.1
got us back to the initial runtime.
Now that we knew that pandas was to blame for the performance degradation, we had to create a 
reproducer in plain pandas to help fix the issue.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = pd.DataFrame(
    {
        &amp;quot;foo&amp;quot;: np.random.randint(1, 50_000, (100_000, )), 
        &amp;quot;bar&amp;quot;: np.random.randint(1, 100_000, (100_000, )),
    },
)

df.groupby(
    &amp;quot;foo&amp;quot;, group_keys=False
).apply(lambda x: x.nlargest(2, columns=&amp;quot;bar&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;group_keys=False&lt;/code&gt;: approx. 11 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;group_keys=True&lt;/code&gt;: approx. 15 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experimenting a bit showed us that the bottleneck got even worse while increasing the number of 
groups during the &lt;code&gt;groupby&lt;/code&gt; calculation. We settled on this version which is 30% slower with 
&lt;code&gt;group_keys=True&lt;/code&gt;, enough to be able to troubleshoot the problem. There was no obvious reason
why the changed value should bring a significant slowdown.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%prun&lt;/code&gt; showed us that the time was almost exclusively spent in a post-processing step that
combines all groups via &lt;code&gt;concat&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="addressing-the-performance-degradation"&gt;Addressing the performance degradation&lt;/h2&gt;
&lt;p&gt;Let's look at how both cases differ. The new version passes the grouping levels to &lt;code&gt;concat&lt;/code&gt;, which
are used to construct the resulting Index levels. This shouldn't be that slow though. Investigations
showed that this runs through a code-path that is very inefficient!&lt;/p&gt;
&lt;p&gt;Looking closer at the results of &lt;code&gt;%prun&lt;/code&gt; pointed us to one specific loop that took up most of
the runtime. This loop calculates the &lt;code&gt;codes&lt;/code&gt; for the resulting index based on the provided 
levels. It's slow, really slow! Every single element provided as &lt;code&gt;keys&lt;/code&gt;, which 
represent the number of groups, is checked against the whole level, which explains our previous 
observation that the runtime got worse with an increasing number of groups. You can check out the 
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#more-concatenating-with-group-keys"&gt;pandas user guide&lt;/a&gt;
if you are interested in situations where this is useful. Fortunately, we have a convenient
advantage in case of &lt;code&gt;groupby&lt;/code&gt;. We know beforehand that every key equals the specific level. 
We added a fast-path that exploits this knowledge getting the runtime of this step more or less to
zero.&lt;/p&gt;
&lt;p&gt;This change resulted in a &lt;a href="https://github.com/pandas-dev/pandas/pull/53195"&gt;small PR&lt;/a&gt; that cut 
the runtime of &lt;code&gt;group_keys=True&lt;/code&gt; to approximately 11 seconds as well.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Now that we made our pandas reproducer run 30% faster, we have to check whether we accomplished our
initial objective. Re-running the local Dask reproducer should give us an idea about the
impact on Dask. We got the performance down to 22 seconds as well! Promising news that saved
our plans for the evening!&lt;/p&gt;
&lt;p&gt;Unfortunately, we had to wait until pandas 2.0.2 was released to run a proper benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="../images/dask_upstream_performance_tests/benchmark_after.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This looks great! Our small pandas change translated to our Dask query and got performance back
to the previous level!&lt;/p&gt;</content><category term="posts"></category><category term="dask"></category><category term="performance"></category><category term="coiled"></category><category term="pandas"></category></entry></feed>